{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated matrix factorization\n",
    "\n",
    "In this notebook we build a federated version of the well-known SVD recommender system introduced by [Simon Funk](https://sifter.org/~simon/journal/20061211.html) during the Netflix Prize. In the following we give a short description of the algorithm and point towards the relevant papers to develop a federated version.\n",
    "\n",
    "The input data is a matrix $r_{iu}$, dubbed the item-user matrix, which contains the ratings that users (indexed by $u, v$) have given to different items (indexed by $i, j$). Since not all the users have rated every item, this matrix is typically large and very sparse (with missing values). The goal is to fill in the missing values given the information we have. This provides a dense matrix $\\hat r_{iu}$ which contains an estimate of how users would rate items they have not yet interacted with. This information can then be used to give recommendations.\n",
    "\n",
    "In this notebook, we will consider a simple case in which the guess for $\\hat r_{iu}$ is given by\n",
    "$$\n",
    "\\hat r_{iu} = \\mu + q_i\\cdot p_u\n",
    "$$\n",
    "where $\\mu$ is the mean value of $r_{ui}$. For fixed item $i$ and user $u$, $q_i, p_u$ are vectors in $\\mathbb R^f$, that have to be estimated based on the known $r_{iu}$. Here $f$ is an integer that defines the model.\n",
    "\n",
    "In order to estimate $q_i, p_u$ we minimize the (L2-regularized) squared error function\n",
    "$$\n",
    "J(q, p) = \\frac{1}{2}\\sum_{(i,u)\\in \\mathcal K} e_{iu}^2 + \\frac{1}{2}\\sum_{i=1}^N \\lambda^{(q)}_i ||q_i||^2 + \\frac{1}{2}\\sum_{u=1}^M  \\lambda^{(p)}_u||p_u||^2 \\,,\n",
    "$$\n",
    "with\n",
    "$$\n",
    "e_{iu} = r_{iu} - \\mu - q_i\\cdot p_u\n",
    "$$\n",
    "and where $\\mathcal K$ is the set of tuples $(i,u)$ for which $r_{iu}$ is known and $ \\lambda^{(q)}_i, \\,\\lambda^{(p)}_u$ are hyper-parameters. \n",
    "Two popular approaches to optimize $J(q,p)$ are Alternating Least Squares (ALS) and Stochastic Gradient Descent (SGD).\n",
    "\n",
    "In this notebook we implement both the standard SGD as well as a federated version. The latter is a mixture between the implementations described in [1711.07638](https://arxiv.org/abs/1711.07638) and in [1901.09888](https://arxiv.org/abs/1901.09888). In particular, we implement an *explicit* SVD including Laplace noise in the gradients computed by each client. As it stands, the server knows which items the user has interacted with but does not know either the rating or the client's  latent factors. It would be interesting to include a Permanent Randomized Response (PRR) mechanism to hide the items the user has interacted with, as done in [1711.07638](https://arxiv.org/abs/1711.07638) (see also the notebook on [PRR and the average attack](./differential_privacy_binary_average_attack.ipynb)).\n",
    "\n",
    "In the following we use the [MovieLens 100k dataset](https://grouplens.org/datasets/movielens/100k/) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shfl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "path = \"data/u.data\"\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.isfile(path):\n",
    "    url = \"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n",
    "    r = requests.get(url)\n",
    "    open(path, \"wb\").write(r.content)\n",
    "df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"userid\", \"itemid\", \"rating\", \"timestamp\"]) \\\n",
    "        .drop(\"timestamp\", axis=1)\n",
    "# Both userid and itemid are integers from 0 to nUsers and nItems respectively\n",
    "df[\"itemid\"] = df[\"itemid\"] - 1\n",
    "df[\"userid\"] = df[\"userid\"] - 1\n",
    "\n",
    "users = np.unique(df.userid.values)\n",
    "items = np.unique(df.itemid.values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the DataBase class that splits the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.data_base import data_base as db\n",
    "\n",
    "class DataBaseSVD(db.DataBase):\n",
    "    \n",
    "    def load_data(self, df, p=0.8):\n",
    "        \"\"\"\n",
    "        Splits the data intro train and test. The only non-null attributes of the class are train_data and test_data.\n",
    "        All the attributes are arrays. The columns are: userid | itemid | rating\n",
    "        \n",
    "        # Arguments: \n",
    "            df: Dataframe of the form: userid | itemid | rating\n",
    "                Both the userid and itemid are integers from 0 to the number of users and items\n",
    "            p: float in [0,1]. Proportion of the data that goes into the training set\n",
    "        \"\"\"\n",
    "        nObs = df.shape[0]\n",
    "        data = df.sample(frac=1)\n",
    "        train_data = df[:math.floor(nObs*p)]\n",
    "        nTrain = train_data.shape[0]\n",
    "        test_data = df[math.floor(nObs*p):]\n",
    "        nTest = test_data.shape[0]\n",
    "        self._train_data = np.array(train_data)\n",
    "        self._train_labels = np.zeros(nTrain, dtype=np.int64)\n",
    "        self._test_data = np.array(test_data)\n",
    "        self._test_labels = np.zeros(nTest, dtype=np.int64)\n",
    "        self._validation_data = np.array([], dtype=np.int64).reshape(0,3)\n",
    "        self._validation_labels = np.array([], dtype=np.int64)\n",
    "\n",
    "DB = DataBaseSVD()\n",
    "DB.load_data(df, p=0.8)\n",
    "DB.train[0]\n",
    "# DB.test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.data_distribution import DataDistribution\n",
    "\n",
    "class DataDistributionSVD(DataDistribution):\n",
    "    \"\"\"\n",
    "    Implementation of the data distribution adapted to the case of SVD.\n",
    "    \"\"\"\n",
    "    def make_data_federated(self, data, labels, num_nodes, percent, weights):\n",
    "        \"\"\"\n",
    "        Method that makes data federated in an iid scenario.\n",
    "        \"\"\"\n",
    "\n",
    "        # Shuffle data\n",
    "        randomize = np.arange(data.shape[0])\n",
    "        np.random.shuffle(randomize)\n",
    "        data = data[randomize,]\n",
    "        \n",
    "        # Select percent\n",
    "        nObs = data.shape[0]\n",
    "        data = data[0:int(percent * nObs / 100), ]\n",
    "        labels = labels[0:int(percent * nObs / 100)]\n",
    "        \n",
    "        num_nodes = len(np.unique(data[:,0]))\n",
    "\n",
    "        federated_data = []\n",
    "        federated_label = []\n",
    "        for user in range(0, num_nodes):\n",
    "            federated_data.append(data[data[:,0]==user])\n",
    "            federated_label.append(labels[data[:,0]==user])\n",
    "        \n",
    "        federated_data = np.array(federated_data)\n",
    "        federated_label = np.array(federated_label)\n",
    "\n",
    "        return federated_data, federated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData = DataDistributionSVD(DB)\n",
    "num_nodes = len(users)\n",
    "federated_data, test_data, test_label = distData.get_federated_data(num_nodes=num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the data is split correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nodes: \" + str(federated_data.num_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.private.query import IdentityFunction\n",
    "from shfl.private.data import DataAccessDefinition\n",
    "\n",
    "data_access_definition = DataAccessDefinition(query=IdentityFunction())\n",
    "federated_data.configure_data_access(data_access_definition)\n",
    "group_query = federated_data.query()\n",
    "first_node = group_query[9]\n",
    "print(first_node.data.shape)\n",
    "print(first_node.label.shape)\n",
    "\n",
    "print(first_node.data[0:5])\n",
    "print(first_node.label[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.model.model import TrainableModel\n",
    "\n",
    "class ClientComputation(TrainableModel):\n",
    "    \"\"\"\n",
    "    # Attributes:\n",
    "        nItems: positive int. Total number of items\n",
    "        f: positive int. Dimension of latent space.\n",
    "        lam: positive float. Regularization parameter.\n",
    "        g: positive float. SGD step.\n",
    "        \n",
    "        q_items: array of shape (nItems, f). Items' latent factors.\n",
    "\n",
    "        userid: int\n",
    "        nObs: number of ratings for a particular user\n",
    "        mu: mean of the user's ratings\n",
    "        p: An array of f elements. User's latent factors. \n",
    "        grad: A dictionary {itemid: array of length f} that contains the gradients for each item.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nItems, f, lam, g, sensitivity, epsilon):\n",
    "        self._nItems = nItems\n",
    "        self._f = f\n",
    "        self._lam = lam\n",
    "        self._g = g\n",
    "        self._sensitivity = sensitivity\n",
    "        self._epsilon = epsilon\n",
    "    \n",
    "    def train(self, data_user, label_user):\n",
    "        \"\"\"\n",
    "        Computes the user's  latent factors and the gradients.\n",
    "        \"\"\"\n",
    "        nObs = len(data_user)\n",
    "        mu = 3 # Rethink this \n",
    "        p = np.random.rand(self._f)\n",
    "        grad = None\n",
    "        \n",
    "        q_items = self._q_items\n",
    "        \n",
    "        if nObs > 0:\n",
    "            mu = np.mean(data_user[:,2])\n",
    "            # LATENT FACTORS\n",
    "            mat = self._lam * nObs * np.identity(self._f)\n",
    "            vec = np.zeros(self._f)\n",
    "            for obs in range(nObs):\n",
    "                q = q_items[data_user[obs,1],:]\n",
    "                mat += np.outer(q, q)\n",
    "                vec += q * (data_user[obs, 2] - mu)\n",
    "            p = np.matmul(np.linalg.inv(mat), vec)\n",
    "            \n",
    "            # GRADIENTS\n",
    "            grad = dict.fromkeys(data_user[:,1]) # dictionary to save the gradients\n",
    "            for obs in range(nObs):\n",
    "                item = data_user[obs, 1]\n",
    "                q = q_items[item,:]\n",
    "                e = data_user[obs, 2] - mu - np.dot(q, p)\n",
    "                grad[item] = self._g * (e * p - self._lam * q)\n",
    "            \n",
    "            # save all the attributes\n",
    "            self._userid = data_user[0, 0]\n",
    "            self._nObs = nObs\n",
    "            self._mu = mu\n",
    "            self._p = p\n",
    "            self._grad = grad\n",
    "        \n",
    "    def predict(self, data_test):\n",
    "        \"\"\"\n",
    "        The input data_test is of the form: userid | itemid | rating.\n",
    "        \"\"\"\n",
    "        data_test = data_test[data_test[:,0]==self._userid]\n",
    "        sum_square_error = 0\n",
    "        nObs_test = data_test.shape[0]\n",
    "        if nObs_test > 0:\n",
    "            for obs in range(nObs_test):\n",
    "                e = data_test[obs, 2] - self._mu - np.dot(self._q_items[data_test[obs, 1],:], self._p)\n",
    "                sum_square_error += e * e\n",
    "        return sum_square_error, nObs_test\n",
    "        \n",
    "    def get_model_params(self):\n",
    "        return self._grad\n",
    "    \n",
    "    def set_model_params(self, q_items):\n",
    "        self._q_items = q_items\n",
    "        \n",
    "    def evaluate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.private.query import Query\n",
    "\n",
    "class LaplaceDictionary(Query):\n",
    "    \"\"\"\n",
    "    Implements the Laplace mechanism where the input is a dictionary and the noise is added to the values.\n",
    "    \n",
    "    # Arguments:\n",
    "        sensitivity: float representing sensitivity of the applied query\n",
    "        epsilon: float for the epsilon you want to apply\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, sensitivity, epsilon):\n",
    "        self._sensitivity = sensitivity\n",
    "        self._epsilon = epsilon\n",
    "    \n",
    "    def get(self, data_dict):\n",
    "        b = self._sensitivity/self._epsilon\n",
    "        for key in data_dict.keys():\n",
    "            data_dict[key] += np.random.laplace(loc=0.0, scale=b, size=len(data_dict[key]))\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class that handles how to bring the gradients together to update the items' latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_aggregator.federated_aggregator import FederatedAggregator\n",
    "\n",
    "class GradientsAggregator(FederatedAggregator):\n",
    "    \"\"\"\n",
    "    Implementation of the class that uses the gradients computed by each client\n",
    "    to update the latent factor of the items.\n",
    "    \"\"\"\n",
    "    def aggregate_weights(self, nItems, q_items, clients_params):\n",
    "        nUsers = len(clients_params)\n",
    "        \n",
    "        for item in range(nItems):\n",
    "            for user in range(nUsers):\n",
    "                if item in clients_params[user].keys():\n",
    "                    q_items[item,:] += clients_params[user][item]\n",
    "                    \n",
    "        return q_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.learning_approach.learning_approach import LearningApproach\n",
    "from shfl.differential_privacy.dp_mechanism import LaplaceMechanism\n",
    "\n",
    "class FederatedSVD(LearningApproach):\n",
    "    \n",
    "    def train_all_clients(self):\n",
    "        for data_node in self._federated_data:\n",
    "            data_node.train_model()\n",
    "    \n",
    "    def get_global_rmse(self, data_test):\n",
    "        rmse_mean = 0\n",
    "        sum_square_error = 0\n",
    "        nObs = 0\n",
    "        for data_node in self._federated_data:\n",
    "            sum_square_error += data_node.predict(data_test)[0]\n",
    "            nObs += data_node.predict(data_test)[1]\n",
    "        global_rmse = np.sqrt(sum_square_error/ nObs)\n",
    "        return global_rmse\n",
    "\n",
    "    def aggregate_weights(self, nItems, q_items):\n",
    "        weights = []\n",
    "        for data_node in self._federated_data:\n",
    "            weights.append(data_node.query_model_params()) # the data in the nodes is accessed using a query\n",
    "        aggregated_weights = self._aggregator.aggregate_weights(nItems, q_items, weights)\n",
    "        self._model.set_model_params(aggregated_weights)\n",
    "        for data_node in self._federated_data:  # At this point we update the q's of every node  \n",
    "            data_node.set_model_params(aggregated_weights)\n",
    "\n",
    "    def run_rounds(self, n, test_data, verbose=False):\n",
    "        nItems = self._model._nItems\n",
    "        f = self._model._f\n",
    "        sensitivity = self._model._sensitivity\n",
    "        epsilon = self._model._epsilon\n",
    "        \n",
    "        if sensitivity is not None and epsilon is not None:\n",
    "            access = DataAccessDefinition(query=LaplaceDictionary(sensitivity=sensitivity, epsilon=epsilon))\n",
    "        else:\n",
    "            access = DataAccessDefinition(query=IdentityFunction())\n",
    "        \n",
    "        q_items = np.random.rand(nItems, f)  # random (but common) initialization\n",
    "        for data_node in self._federated_data:\n",
    "            data_node.configure_model_params_access(data_access_definition=access) # include DP if necessary\n",
    "            data_node.set_model_params(q_items) # initialize\n",
    "        if verbose: print(\"Global Test RMSE:\")\n",
    "        for i in range(n):\n",
    "            self.train_all_clients()\n",
    "            self.aggregate_weights(nItems, q_items)\n",
    "            if verbose: print(\"\\t Iteration \" + str(i+1) + \": \" + str(round(self.get_global_rmse(test_data), 3)))\n",
    "        return self.get_global_rmse(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by computing the result without any differential privacy. In this case, the only difference with the non-federated model is that the value of $\\mu$ is computed for each client independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder():\n",
    "    model = ClientComputation(nItems=1682, f=10, lam=0.1, g=0.01, sensitivity=None, epsilon=None)\n",
    "    return model\n",
    "\n",
    "fedSVD = FederatedSVD(model_builder, federated_data, aggregator=GradientsAggregator())\n",
    "result = fedSVD.run_rounds(15, test_data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of differential privacy\n",
    "\n",
    "In order to check the performance of the SVD when we add differential privacy, we start by computing the RMSE obtained using several different methods.\n",
    "\n",
    "### Random ratings\n",
    "\n",
    "We start by computing the RMSE that we get from filling the ratings matrix randomly with a number between 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings = DB.test[0][:,2]\n",
    "RMSE_random = np.sqrt(np.mean(np.square(test_ratings - np.random.uniform(1,5, len(test_ratings)))))\n",
    "print(\"RMSE using a random rating: \" + str(round(RMSE_random,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(DB.train[0][:,2])\n",
    "print(\"Mean rating: \" + str(round(train_mean,2)))\n",
    "test_ratings = DB.test[0][:,2]\n",
    "RMSE_mean = np.sqrt(np.mean(np.square(test_ratings - train_mean)))\n",
    "print(\"RMSE using the mean rating: \" + str(round(RMSE_mean,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Federated SVD\n",
    "\n",
    "In this case, we include a non-federated SVD optimized using a simple Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "import cython\n",
    "\n",
    "def SVD_SGD(data_train, data_test, f, lam, g, nIter):\n",
    "    \n",
    "    cdef int nItems, nUsers, f_c, nIter_c, nObs_train, nObs_test\n",
    "    cdef float lam_c, g_c, mu\n",
    "\n",
    "    data_full = np.concatenate((data_train, data_test), axis=0)\n",
    "    nUsers = len(np.unique(data_full[:,0]))\n",
    "    nItems = len(np.unique(data_full[:,1]))\n",
    "    \n",
    "    data_train = np.array(data_train, dtype=long)\n",
    "    data_test = np.array(data_test, dtype=long)\n",
    "    \n",
    "    cdef np.ndarray[np.long_t, ndim=2] data_train_c, data_test_c\n",
    "    cdef np.ndarray[np.double_t, ndim=2] q_items, p_users\n",
    "    \n",
    "    f_c, lam_c, g_c, nIter_c = f, lam, g, nIter\n",
    "    data_train_c = data_train\n",
    "    data_test_c = data_test\n",
    "\n",
    "    mu = np.mean(data_train[:,2])\n",
    "    nObs_train = data_train.shape[0]\n",
    "    q_items = np.random.rand(nItems, f_c)\n",
    "    p_users = np.random.rand(f_c, nUsers)\n",
    "    \n",
    "    cdef double q, p, qp, e\n",
    "    \n",
    "    # TRAIN\n",
    "    for i in range(nIter_c):\n",
    "        for obs in range(nObs_train):\n",
    "            qp = 0\n",
    "            for k in range(f_c):\n",
    "                qp += q_items[data_train_c[obs, 1],k] * p_users[k,data_train_c[obs, 0]]\n",
    "            e = data_train_c[obs, 2] - mu - qp\n",
    "            for k in range(f_c):\n",
    "                q = q_items[data_train_c[obs, 1],k]\n",
    "                p = p_users[k,data_train_c[obs, 0]]\n",
    "                q_items[data_train_c[obs, 1],k] = q + g_c * (e * p - lam_c * q)\n",
    "                p_users[k,data_train_c[obs, 0]] = p + g_c * (e * q - lam_c * p)\n",
    "    \n",
    "    # EVALUATE\n",
    "    nObs_test = data_test.shape[0]\n",
    "    RMSE = 0\n",
    "    for obs in range(nObs_test):\n",
    "        qp = 0\n",
    "        for k in range(f_c):\n",
    "            qp += q_items[data_test_c[obs, 1],k] * p_users[k,data_test_c[obs, 0]]\n",
    "        e = data_test_c[obs, 2] - mu - qp\n",
    "        RMSE += e * e\n",
    "    RMSE = np.sqrt(RMSE / nObs_test)\n",
    "    \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_nonF = SVD_SGD(data_train=DB.train[0], data_test=DB.test[0], f=10, lam=0.1, g=0.01, nIter=200)\n",
    "print(\"RMSE for the non-federated SVD: \" + str(round(RMSE_nonF, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentially private SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for different values of epsilon and check how the performance (in terms of the RMSE) decreases as the privacy increases. In this case we simply fix the sensitivity to $\\Delta f = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 15\n",
    "epsilon_range = np.array([0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 2, 3])\n",
    "RMSE = np.zeros(len(epsilon_range))\n",
    "\n",
    "for i in range(len(epsilon_range)):\n",
    "    def model_builder():\n",
    "        model = ClientComputation(nItems=1682, f=10, lam=0.1, g=0.01, sensitivity=0.01, epsilon=epsilon_range[i])\n",
    "        return model\n",
    "    \n",
    "    fedSVD = FederatedSVD(model_builder, federated_data, aggregator=GradientsAggregator())\n",
    "    result = fedSVD.run_rounds(n_iter, test_data)\n",
    "    RMSE[i] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the RMSE as a function of $\\epsilon$. The horizontal line corresponds to the RMSE obtained using the mean rating in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "plt.plot(epsilon_range, RMSE, label=\"Laplace\")\n",
    "plt.axhline(y=RMSE_random, color='r', linestyle='--', label=\"random\", lw=2)\n",
    "plt.axhline(y=RMSE_mean, color='orange', linestyle='--', label=\"mean\", lw=2)\n",
    "plt.axhline(y=RMSE_nonF, color='green', linestyle='--', label=\"non-federated\", lw=2)\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('RMSE')\n",
    "ax.set_xlim([-0.1, max(epsilon_range)])\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sherpa_venvPy3",
   "language": "python",
   "name": "sherpa_venvpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
