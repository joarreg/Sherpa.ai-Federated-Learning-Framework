{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised federated learning: K-means clustering\n",
    "\n",
    "The present notebook tackles the problem of *unsupervised* learning in a federated configuration. \n",
    "In particular, a K-Means clustering is used from the `sklearn` library (see [this link](https://scikit-learn.org/stable/modules/clustering.html#k-means)).\n",
    "\n",
    "## Model definition\n",
    "In order to make the model interact with the Federated Learning platform we will simply need to define the following:\n",
    "1. How to load the data;\n",
    "2. The model.\n",
    "3. How to aggregate model's parameters from each federated node. \n",
    "\n",
    "In the following, each step is described for the case of a 2D clustering. \n",
    "\n",
    "**How to load the data**\\\n",
    "A method that returns train, test and validation data need to be provided, wrapping it in the class `data_base`.\\\n",
    "Typically, existing data is used. \n",
    "However, in this example a series of 2D points is created for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shfl\n",
    "from shfl.data_base import data_base as db\n",
    "\n",
    "# Create database: \n",
    "class ClusteringDB(db.DataBase):\n",
    "    \"\"\"\n",
    "    Create clusters of 2D points\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Change these parameters to modify dataset\n",
    "        \"\"\"\n",
    "        # True centers of clusters:\n",
    "        self._mean = np.array([[12, 10],\n",
    "                               [10, 15],\n",
    "                               [15, 15]])\n",
    "        # Covariance for each cluster: [s_xx, s_xy, s_yx, s_yy]\n",
    "        self._cov = np.array([[3, 0, 0, 3],\n",
    "                              [2, 0, 0, 2],\n",
    "                              [4, 0, 0, 4]])\n",
    "        # Size of datasets:\n",
    "        self._size_train = 200\n",
    "        self._size_test = 300\n",
    "        self._size_validation = 20\n",
    "        \n",
    "    def _create_data(self, size_data):\n",
    "        data = np.empty((0, 3))\n",
    "        size_cluster = round(size_data / (self._mean).shape[0])\n",
    "        for i_cluster in range((self._mean).shape[0]):\n",
    "            i_data = np.random.multivariate_normal( \\\n",
    "                        self._mean[i_cluster,:], self._cov[i_cluster,:].reshape(2,2), size_cluster)\n",
    "            # Add column for label:\n",
    "            i_data = np.concatenate([i_data, \\\n",
    "                        np.ones((i_data.shape[0],1), dtype=i_data.dtype) * i_cluster], axis=1)\n",
    "            # Append cluster data:\n",
    "            data = np.append(data, i_data, axis = 0)\n",
    "        return(data[:, [0, 1]], data[:, 2]) \n",
    "       \n",
    "    def load_data(self):\n",
    "        self._train_data, self._train_labels = self._create_data(self._size_train)\n",
    "        self._test_data, self._test_labels = self._create_data(self._size_test)\n",
    "        self._validation_data, self._validation_labels = self._create_data(self._size_validation)\n",
    "        self.shuffle()\n",
    "        return self.data\n",
    "\n",
    "# Assign database:    \n",
    "database = ClusteringDB()\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# Visualize train data: \n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=30, cmap=\"tab20\")\n",
    "plt.scatter(database._mean[:, 0], database._mean[:, 1], c=[0,1,2], marker='*', s=200, cmap=\"tab20\", label = \"Mean values\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.legend(title = \"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model** <br/>\n",
    "Now we will define the model and wrap it in the class `TrainableModel`. \n",
    "Abstract methods need to be defined, i.e. we must provide functions for `train`, `predict`, `evaluate`, `get_parameters` and `set_parameters`. \n",
    "In the case of 2D clustering, a possible implementation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from shfl.model import TrainableModel\n",
    "\n",
    "class K_Means2D(TrainableModel):\n",
    "    \n",
    "    def __init__(self, n_clusters, init, n_init=1):\n",
    "        self._k_means = KMeans(n_clusters=n_clusters, init=init, n_init=n_init)\n",
    "        self._k_means.cluster_centers_ = init\n",
    "        \n",
    "    def train(self, data, labels = None):\n",
    "        self._k_means.fit(data)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        predicted_labels = self._k_means.predict(data)\n",
    "        return(predicted_labels)\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        \"\"\"\n",
    "        Add here all the metrics to evaluate the performance \n",
    "        \"\"\"\n",
    "        prediction = self.predict(data)\n",
    "    \n",
    "        homo = metrics.homogeneity_score(labels, prediction) \n",
    "        compl = metrics.completeness_score(labels, prediction)\n",
    "        v_meas = metrics.v_measure_score(labels, prediction)\n",
    "        rai = metrics.adjusted_rand_score(labels, prediction)\n",
    "        return homo, compl, v_meas, rai\n",
    "\n",
    "    def get_model_params(self):\n",
    "        return self._k_means.cluster_centers_\n",
    "    \n",
    "    def set_model_params(self, params):\n",
    "        \"\"\"\n",
    "        New model with initial centers\n",
    "        \"\"\"\n",
    "        n_clusters = params.shape[0]\n",
    "        self.__init__(n_clusters=n_clusters, init=params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means(KMeans_model, data, title):\n",
    "    # Step size of the mesh. Smaller it is, better the quality\n",
    "    h = .02 \n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = KMeans_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.tab20,\n",
    "               aspect='auto', origin='lower')\n",
    "    # Plot data:\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=40, color = 'k', marker='.')\n",
    "    \n",
    "    # Plot the centroids\n",
    "    centroids = KMeans_model.get_model_params()\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='*', s=200, linewidths=2,\n",
    "                color='black', zorder=10)\n",
    "    plt.title(title)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot train data:\n",
    "model_centralized = K_Means2D(n_clusters=3, init = database._mean)\n",
    "model_centralized.train(train_data)\n",
    "print(model_centralized.get_model_params())\n",
    "plot_k_means(model_centralized, train_data, title = \"Benchmark: K-means using Centralized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to aggregate model's parameters from each federated node** <br/>\n",
    "Next, we define the aggregation of the federated outputs to be the average. \n",
    "Since the labels of clusters can vary among each node, we cannot average the centroids right away. \n",
    "A solution is to choose the lowest distance average: this is achieved by simply applying the k-means algorithm on the centroids coordinates of all nodes. \n",
    "The class `ClusterAvgFedAggregator` below shows the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_aggregator.federated_aggregator import FederatedAggregator\n",
    "\n",
    "class ClusterAvgFedAggregator(FederatedAggregator):\n",
    "    \"\"\"\n",
    "    Implementation of Cluster Average Federated Aggregator. \n",
    "    It adds another k-means to find the minimum distance of cluster centroids coming from each node. \n",
    "    \"\"\"\n",
    "\n",
    "    def aggregate_weights(self, clients_params):\n",
    "        clients_params_array = np.concatenate((clients_params))\n",
    "        \n",
    "        n_clusters = clients_params[0].shape[0]\n",
    "        model_aggregator = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "        model_aggregator.fit(clients_params_array)\n",
    "        aggregated_weights = np.array(model_aggregator.cluster_centers_)\n",
    "        return aggregated_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model in a Federated configuration\n",
    "We are now ready to run our model in a federated configuration. \n",
    "We distribute the data over the nodes, assuming the data is IID.\\\n",
    "The performance is assessed by several clustering metrics (see [this link](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)).\\\n",
    "For reference, below we compare the metrics of:\n",
    " - Each node; \n",
    " - The global (federated) model;\n",
    " - The centralized (non-federated) model.\n",
    " \n",
    "It can be observed that the performance of *Global federated model* is in general superior with respect to the performance of each node, thus the federated learning approach proves to be beneficial. Moreover, the performance of the Global federated model is very close to the performance of the centralized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.learning_approach.federated_government import FederatedGovernment\n",
    "\n",
    "# Use this to Plot after each Aggregation:\n",
    "class PlotFederatedGovernment(FederatedGovernment):\n",
    "    def evaluate_global_model(self, data_test, label_test):\n",
    "        evaluation = self._model.evaluate(data_test, label_test)\n",
    "        print(\"Global model test performance : \" + str(evaluation))\n",
    "        plot_k_means(self._model, data_test, title = \"Global model on Test data: K-means using FEDERATED data\")\n",
    "\n",
    "        \n",
    "n_clusters = 3 # Set number of clusters\n",
    "def model_builder():\n",
    "    model = K_Means2D(n_clusters=n_clusters, init=np.zeros((n_clusters, 2)))\n",
    "    return model\n",
    "\n",
    "# Create the IID data: \n",
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_label = iid_distribution.get_federated_data(num_nodes = 12, percent=100)\n",
    "print(\"Number of nodes: \" + str(federated_data.num_nodes()))\n",
    "\n",
    "# Run the algorithm:\n",
    "aggregator = ClusterAvgFedAggregator()\n",
    "federated_government = PlotFederatedGovernment(model_builder, federated_data, aggregator)\n",
    "print(\"Test data size: \" + str(test_data.shape[0]))\n",
    "print(\"\\n\")\n",
    "federated_government.run_rounds(n = 6, test_data = test_data, test_label = test_label)\n",
    "\n",
    "# Reference Centralized (non federate) model:\n",
    "print(\"Centralized model test performance : \" + str(model_centralized.evaluate(data=test_data, labels=test_labels)))\n",
    "plot_k_means(model_centralized, test_data, title = \"Benchmark on Test data: K-means using CENTRALIZED data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
