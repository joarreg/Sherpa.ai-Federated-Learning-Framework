{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sherpa.ai Federated Learning Framework Sherpa.ai Federated Learning Framework has been developed to facilitate open research in the \ufb01eld, with the objective of building models that learn from decentralized data, preserving data privacy. It is an open-source platform and aims at supporting 100 percent of the AI algorithms used in industry. Sherpa.ai Federated Learning Framework is an open-source framework for Machine Learning that is dedicated to data privacy protection. It has been developed to facilitate open research and experimentation in Federated Learning, a machine learning paradigm aimed at learning models from decentralized data (e.g., data located on users\u2019 smartphones) and ensuring data privacy. This is achieved by training the model locally in each node (e.g., on each smartphone), sharing the model-updated parameters (not the data) and securely aggregating the updated parameters to build a better model. This technology could be disruptive in cases where it is compulsory to ensure data privacy, as in the following examples: When data contains sensitive information, such as email accounts, personalized recommendations, and health information, applications should employ data privacy mechanisms to learn from a population of users whilst the sensitive data remains on each user\u2019s device. When data is located in data silos, an automotive parts manufacturer, for example, may be reluctant to disclose their data, but would benefit from models that learn from each other's data, in order to improve production and supply chain management. Due to data-privacy legislation, banks and telecom companies, for example, cannot share individual records, but would benefit from models that learn from data across several entities. Sherpa.ai is focused on democratizing Federated Learning by providing methodologies, pipelines, and evaluation techniques specifically designed for Federated Learning. The Sherpa.ai Federated Learning Platform enables developers to simulate Federated Learning scenarios with models, algorithms, and data provided by the framework, as well as their own data. Sherpa.ai Federated Learning Framework is a project of Sherpa.ai in collaboration with the Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI) research group from the University of Granada . Installation See the install documentation for instructions on how to install Sherpa.ai Federated Learning Framework. Getting Started See the get started documentation for a brief introduction to using Sherpa.ai Federated Learning Framework. Contributing If you are interested in contributing to Sherpa.ai Federated Learning Framework with tutorials, datasets, models, aggregation mechanisms or any other code others could benefit of please be sure to review the contributing guidelines . Issues Use GitHub issues for tracking requests and bugs. Questions Please direct questions to Sherpa Developers Slack .","title":"Home"},{"location":"#sherpaai-federated-learning-framework","text":"Sherpa.ai Federated Learning Framework has been developed to facilitate open research in the \ufb01eld, with the objective of building models that learn from decentralized data, preserving data privacy. It is an open-source platform and aims at supporting 100 percent of the AI algorithms used in industry. Sherpa.ai Federated Learning Framework is an open-source framework for Machine Learning that is dedicated to data privacy protection. It has been developed to facilitate open research and experimentation in Federated Learning, a machine learning paradigm aimed at learning models from decentralized data (e.g., data located on users\u2019 smartphones) and ensuring data privacy. This is achieved by training the model locally in each node (e.g., on each smartphone), sharing the model-updated parameters (not the data) and securely aggregating the updated parameters to build a better model. This technology could be disruptive in cases where it is compulsory to ensure data privacy, as in the following examples: When data contains sensitive information, such as email accounts, personalized recommendations, and health information, applications should employ data privacy mechanisms to learn from a population of users whilst the sensitive data remains on each user\u2019s device. When data is located in data silos, an automotive parts manufacturer, for example, may be reluctant to disclose their data, but would benefit from models that learn from each other's data, in order to improve production and supply chain management. Due to data-privacy legislation, banks and telecom companies, for example, cannot share individual records, but would benefit from models that learn from data across several entities. Sherpa.ai is focused on democratizing Federated Learning by providing methodologies, pipelines, and evaluation techniques specifically designed for Federated Learning. The Sherpa.ai Federated Learning Platform enables developers to simulate Federated Learning scenarios with models, algorithms, and data provided by the framework, as well as their own data. Sherpa.ai Federated Learning Framework is a project of Sherpa.ai in collaboration with the Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI) research group from the University of Granada .","title":"Sherpa.ai Federated Learning Framework"},{"location":"#installation","text":"See the install documentation for instructions on how to install Sherpa.ai Federated Learning Framework.","title":"Installation"},{"location":"#getting-started","text":"See the get started documentation for a brief introduction to using Sherpa.ai Federated Learning Framework.","title":"Getting Started"},{"location":"#contributing","text":"If you are interested in contributing to Sherpa.ai Federated Learning Framework with tutorials, datasets, models, aggregation mechanisms or any other code others could benefit of please be sure to review the contributing guidelines .","title":"Contributing"},{"location":"#issues","text":"Use GitHub issues for tracking requests and bugs.","title":"Issues"},{"location":"#questions","text":"Please direct questions to Sherpa Developers Slack .","title":"Questions"},{"location":"CONTRIBUTING/","text":"Bug Reporting, Feature Request or Pull Request We are happy to accept contributions in different ways. Bug Reporting If you found a bug or unexpected behaviour in the framework please, follow the next steps to report. Check your code version. Maybe the problem is already fixed in a new version. Search in the opened issues to avoid duplicity. If bug is not covered yet, please, provide as much information as possible about your environment and, if possible, provide some code to reproduce the behavior. If you are able to solve the problem you might propose a solution in a pull request. Feature Request If you are interested in a new feature that is not developed at the moment you can use the issue tracker to request it. Just be sure that you explain in the clearest possible way the new behaviour that you would like. A good option is to provide some pseudocode or schema to clarify the new feature. Pull Requests If you are going to add some code that modifies the software architecture or changes the behavior of a functionality is recommended to describe the changes to discuss and avoid to waste time. If you are just fixing an evident bug it is not necessary. Developing process The following are the basic conventions that we are using in this project. Github Workflow The main strategy that we are using is the \"Feature Branch Workflow\" that is very well described here (in this case for Bitbucket, but the same can be applied for github). Code Style We are using Google Python Style Guide Code Tests All tests have to pass with 100% line coverage. You only need to execute the following command in the base project directory: pytest --cov=shfl test/","title":"Bug Reporting, Feature Request or Pull Request"},{"location":"CONTRIBUTING/#bug-reporting-feature-request-or-pull-request","text":"We are happy to accept contributions in different ways.","title":"Bug Reporting, Feature Request or Pull Request"},{"location":"CONTRIBUTING/#bug-reporting","text":"If you found a bug or unexpected behaviour in the framework please, follow the next steps to report. Check your code version. Maybe the problem is already fixed in a new version. Search in the opened issues to avoid duplicity. If bug is not covered yet, please, provide as much information as possible about your environment and, if possible, provide some code to reproduce the behavior. If you are able to solve the problem you might propose a solution in a pull request.","title":"Bug Reporting"},{"location":"CONTRIBUTING/#feature-request","text":"If you are interested in a new feature that is not developed at the moment you can use the issue tracker to request it. Just be sure that you explain in the clearest possible way the new behaviour that you would like. A good option is to provide some pseudocode or schema to clarify the new feature.","title":"Feature Request"},{"location":"CONTRIBUTING/#pull-requests","text":"If you are going to add some code that modifies the software architecture or changes the behavior of a functionality is recommended to describe the changes to discuss and avoid to waste time. If you are just fixing an evident bug it is not necessary.","title":"Pull Requests"},{"location":"CONTRIBUTING/#developing-process","text":"The following are the basic conventions that we are using in this project.","title":"Developing process"},{"location":"CONTRIBUTING/#github-workflow","text":"The main strategy that we are using is the \"Feature Branch Workflow\" that is very well described here (in this case for Bitbucket, but the same can be applied for github).","title":"Github Workflow"},{"location":"CONTRIBUTING/#code-style","text":"We are using Google Python Style Guide","title":"Code Style"},{"location":"CONTRIBUTING/#code-tests","text":"All tests have to pass with 100% line coverage. You only need to execute the following command in the base project directory: pytest --cov=shfl test/","title":"Code Tests"},{"location":"data_distribution/","text":"Data distribution [source] DataDistribution class shfl.data_distribution.data_distribution.DataDistribution(database) Abstract class for data distribution Arguments: database : Database to distribute. (see: Databases ) DataDistribution methods get_federated_data get_federated_data(num_nodes, percent=100, weights=None, sampling='without_replacement') Method that split the whole data between the established number of nodes. Arguments: num_nodes : Number of nodes to create percent : Percent of the data (between 0 and 100) to be distributed (default is 100) weights : Array of weights for weighted distribution (default is None) sampling : methodology between with or without sampling (default \"without_sampling\") Returns: federated_data, test_data, test_label make_data_federated make_data_federated(data, labels, num_nodes, percent, weights, sampling) Method that must implement every data distribution extending this class Arguments: data : Array of data labels : Labels num_nodes : Number of nodes percent : Percent of the data (between 0 and 100) to be distributed (default is 100) weights : Array of weights for weighted distribution (default is None) sampling : methodology between with or without sampling (default \"without_sampling\") Returns: federated_data : Data for each client federated_label : Labels for each client [source] IidDataDistribution class shfl.data_distribution.data_distribution.IidDataDistribution(database) Implementation of an independent and identically distributed data distribution using Data Distribution [source] NonIidDataDistribution class shfl.data_distribution.data_distribution.NonIidDataDistribution(database) Implementation of a non-independent and identically distributed data distribution using Data Distribution In this data distribution we simulate the scenario in which clients have non-identical distribution since they know partially the total classes of the problem. This distribution only works with classification problems. NonIidDataDistribution methods choose_labels choose_labels(total_labels) Method that randomly choose labels used for each client in non-iid scenario. Arguments: num_nodes : Number of nodes total_labels : Number of labels Returns: labels_to_use","title":"Data Distribution"},{"location":"data_distribution/#data-distribution","text":"[source]","title":"Data distribution"},{"location":"data_distribution/#datadistribution-class","text":"shfl.data_distribution.data_distribution.DataDistribution(database) Abstract class for data distribution Arguments: database : Database to distribute. (see: Databases )","title":"DataDistribution class"},{"location":"data_distribution/#datadistribution-methods","text":"","title":"DataDistribution methods"},{"location":"data_distribution/#get_federated_data","text":"get_federated_data(num_nodes, percent=100, weights=None, sampling='without_replacement') Method that split the whole data between the established number of nodes. Arguments: num_nodes : Number of nodes to create percent : Percent of the data (between 0 and 100) to be distributed (default is 100) weights : Array of weights for weighted distribution (default is None) sampling : methodology between with or without sampling (default \"without_sampling\") Returns: federated_data, test_data, test_label","title":"get_federated_data"},{"location":"data_distribution/#make_data_federated","text":"make_data_federated(data, labels, num_nodes, percent, weights, sampling) Method that must implement every data distribution extending this class Arguments: data : Array of data labels : Labels num_nodes : Number of nodes percent : Percent of the data (between 0 and 100) to be distributed (default is 100) weights : Array of weights for weighted distribution (default is None) sampling : methodology between with or without sampling (default \"without_sampling\") Returns: federated_data : Data for each client federated_label : Labels for each client [source]","title":"make_data_federated"},{"location":"data_distribution/#iiddatadistribution-class","text":"shfl.data_distribution.data_distribution.IidDataDistribution(database) Implementation of an independent and identically distributed data distribution using Data Distribution [source]","title":"IidDataDistribution class"},{"location":"data_distribution/#noniiddatadistribution-class","text":"shfl.data_distribution.data_distribution.NonIidDataDistribution(database) Implementation of a non-independent and identically distributed data distribution using Data Distribution In this data distribution we simulate the scenario in which clients have non-identical distribution since they know partially the total classes of the problem. This distribution only works with classification problems.","title":"NonIidDataDistribution class"},{"location":"data_distribution/#noniiddatadistribution-methods","text":"","title":"NonIidDataDistribution methods"},{"location":"data_distribution/#choose_labels","text":"choose_labels(total_labels) Method that randomly choose labels used for each client in non-iid scenario. Arguments: num_nodes : Number of nodes total_labels : Number of labels","title":"choose_labels"},{"location":"data_distribution/#returns","text":"labels_to_use","title":"Returns:"},{"location":"databases/","text":"Databases The framework provides some tools to use external data and experiment with different datasets without effort. [source] DataBase class shfl.data_base.data_base.DataBase() Abstract class for data base. Load method must be implemented in order to create a database able to interact with the system, in concrete with data distribution methods (see: Data Distribution ). Load method should save data in the protected Attributes: Attributes: train_data, train_labels, test_data, test_labels Properties: train : Returns train data and labels test : Returns test data and labels data : Returns train data, train labels, validation data, validation labels, test data and test labels DataBase methods load_data load_data() Abstract method that loads the data shuffle shuffle() Shuffles all data [source] LabeledDatabase class shfl.data_base.data_base.LabeledDatabase(data, labels, train_percentage=0.8) Class to create generic labeled database from data and labels vectors Arguments data : Data features to load labels : Labels for this features train_percentage : float between 0 and 1 to indicate how much data is dedicated to train [source] Emnist class shfl.data_base.emnist.Emnist() Implementation for load EMNIST data References EMNIST dataset [source] FashionMnist class shfl.data_base.fashion_mnist.FashionMnist() Implementation for load FASHION-MNIST data References FASHION-MNIST dataset [source] CaliforniaHousing class shfl.data_base.data_base.CaliforniaHousing() This database loads the California housing dataset from sklearn, mainly for regression tasks. [source] Iris class shfl.data_base.data_base.Iris() This database loads the Irisdataset from sklearn, mainly for clustering tasks. split_train_test shfl.data_base.data_base.split_train_test(data, labels, dim) Method that randomly choose the train and test sets from data and labels. Arguments: data : Numpy matrix with data for extract the validation data labels : Numpy array with labels dim : Size for validation data Returns: new_data : Data, labels, validation data and validation labels","title":"Databases"},{"location":"databases/#databases","text":"The framework provides some tools to use external data and experiment with different datasets without effort. [source]","title":"Databases"},{"location":"databases/#database-class","text":"shfl.data_base.data_base.DataBase() Abstract class for data base. Load method must be implemented in order to create a database able to interact with the system, in concrete with data distribution methods (see: Data Distribution ). Load method should save data in the protected Attributes: Attributes: train_data, train_labels, test_data, test_labels Properties: train : Returns train data and labels test : Returns test data and labels data : Returns train data, train labels, validation data, validation labels, test data and test labels","title":"DataBase class"},{"location":"databases/#database-methods","text":"","title":"DataBase methods"},{"location":"databases/#load_data","text":"load_data() Abstract method that loads the data","title":"load_data"},{"location":"databases/#shuffle","text":"shuffle() Shuffles all data [source]","title":"shuffle"},{"location":"databases/#labeleddatabase-class","text":"shfl.data_base.data_base.LabeledDatabase(data, labels, train_percentage=0.8) Class to create generic labeled database from data and labels vectors Arguments data : Data features to load labels : Labels for this features train_percentage : float between 0 and 1 to indicate how much data is dedicated to train [source]","title":"LabeledDatabase class"},{"location":"databases/#emnist-class","text":"shfl.data_base.emnist.Emnist() Implementation for load EMNIST data References EMNIST dataset [source]","title":"Emnist class"},{"location":"databases/#fashionmnist-class","text":"shfl.data_base.fashion_mnist.FashionMnist() Implementation for load FASHION-MNIST data References FASHION-MNIST dataset [source]","title":"FashionMnist class"},{"location":"databases/#californiahousing-class","text":"shfl.data_base.data_base.CaliforniaHousing() This database loads the California housing dataset from sklearn, mainly for regression tasks. [source]","title":"CaliforniaHousing class"},{"location":"databases/#iris-class","text":"shfl.data_base.data_base.Iris() This database loads the Irisdataset from sklearn, mainly for clustering tasks.","title":"Iris class"},{"location":"databases/#split_train_test","text":"shfl.data_base.data_base.split_train_test(data, labels, dim) Method that randomly choose the train and test sets from data and labels. Arguments: data : Numpy matrix with data for extract the validation data labels : Numpy array with labels dim : Size for validation data Returns: new_data : Data, labels, validation data and validation labels","title":"split_train_test"},{"location":"federated_aggregator/","text":"Federated aggregator Implementations of different algorithms to aggregate models. [source] FederatedAggregator class shfl.federated_aggregator.federated_aggregator.FederatedAggregator(percentage=None) Interface for Federated Aggregator. Arguments: percentage : Percentage of total data in each client (default None) FederatedAggregator methods aggregate_weights aggregate_weights(clients_params) Abstract method that aggregates the weights of the client models. Arguments: clients_params : Params that represents local clients learning models. Returns: aggregated_weights : Aggregated weights [source] FedAvgAggregator class shfl.federated_aggregator.federated_aggregator.FedAvgAggregator(percentage=None) Implementation of Average Federated Aggregator. It only uses a simple average of the parameters of all the models. It implements Federated Aggregator [source] WeightedFedAvgAggregator class shfl.federated_aggregator.federated_aggregator.WeightedFedAvgAggregator(percentage=None) Implementation of Weighted Federated Avegaring Aggregator. The aggregation of the parameters is based in the number of data in every node. It implements Federated Aggregator [source] IowaFederatedAggregator class shfl.federated_aggregator.iowa_federated_aggregator.IowaFederatedAggregator() Class of the IOWA version of WeightedFedAvgAggregator IowaFederatedAggregator methods set_ponderation set_ponderation(performance, dynamic=True, a=0, b=0.2, c=0.8, y_b=0.4, k=0.75) Method which calculate ponderation weights of each client based on the performance vector. Arguments: performance : vector with the performance of each local client in a validation set dynamic : boolean indicating if we use the dynamic or static version (default True) a : first argument of linguistic quantifier (default 0) b : second argument of linguistic quantifier (default 0.2) c : third argument of linguistic quantifier (default 0.8) y_b : fourth argument of linguistic quantifier (default 0.4) k : distance param of the dynamic version (default 3/4) q_function q_function(x) Method that returns ponderation weights for OWA operator Arguments: x : value of the ordering function u (orderer performance of each local model) Returns: ponderation_weights : ponderation of each client. get_ponderation_weights get_ponderation_weights() Method that returns the value of the linguistic quantifier (Q_function) for each value x Returns: ponderation_weights : ponderation of each client. [source] ClusterFedAvgAggregator class shfl.federated_aggregator.federated_aggregator.ClusterFedAvgAggregator(percentage=None) Implementation of Cluster Average Federated Aggregator. It adds another k-means to find the minimum distance of cluster centroids coming from each node. It implements Federated Aggregator","title":"Federated Aggregator"},{"location":"federated_aggregator/#federated-aggregator","text":"Implementations of different algorithms to aggregate models. [source]","title":"Federated aggregator"},{"location":"federated_aggregator/#federatedaggregator-class","text":"shfl.federated_aggregator.federated_aggregator.FederatedAggregator(percentage=None) Interface for Federated Aggregator. Arguments: percentage : Percentage of total data in each client (default None)","title":"FederatedAggregator class"},{"location":"federated_aggregator/#federatedaggregator-methods","text":"","title":"FederatedAggregator methods"},{"location":"federated_aggregator/#aggregate_weights","text":"aggregate_weights(clients_params) Abstract method that aggregates the weights of the client models. Arguments: clients_params : Params that represents local clients learning models. Returns: aggregated_weights : Aggregated weights [source]","title":"aggregate_weights"},{"location":"federated_aggregator/#fedavgaggregator-class","text":"shfl.federated_aggregator.federated_aggregator.FedAvgAggregator(percentage=None) Implementation of Average Federated Aggregator. It only uses a simple average of the parameters of all the models. It implements Federated Aggregator [source]","title":"FedAvgAggregator class"},{"location":"federated_aggregator/#weightedfedavgaggregator-class","text":"shfl.federated_aggregator.federated_aggregator.WeightedFedAvgAggregator(percentage=None) Implementation of Weighted Federated Avegaring Aggregator. The aggregation of the parameters is based in the number of data in every node. It implements Federated Aggregator [source]","title":"WeightedFedAvgAggregator class"},{"location":"federated_aggregator/#iowafederatedaggregator-class","text":"shfl.federated_aggregator.iowa_federated_aggregator.IowaFederatedAggregator() Class of the IOWA version of WeightedFedAvgAggregator","title":"IowaFederatedAggregator class"},{"location":"federated_aggregator/#iowafederatedaggregator-methods","text":"","title":"IowaFederatedAggregator methods"},{"location":"federated_aggregator/#set_ponderation","text":"set_ponderation(performance, dynamic=True, a=0, b=0.2, c=0.8, y_b=0.4, k=0.75) Method which calculate ponderation weights of each client based on the performance vector. Arguments: performance : vector with the performance of each local client in a validation set dynamic : boolean indicating if we use the dynamic or static version (default True) a : first argument of linguistic quantifier (default 0) b : second argument of linguistic quantifier (default 0.2) c : third argument of linguistic quantifier (default 0.8) y_b : fourth argument of linguistic quantifier (default 0.4) k : distance param of the dynamic version (default 3/4)","title":"set_ponderation"},{"location":"federated_aggregator/#q_function","text":"q_function(x) Method that returns ponderation weights for OWA operator Arguments: x : value of the ordering function u (orderer performance of each local model) Returns: ponderation_weights : ponderation of each client.","title":"q_function"},{"location":"federated_aggregator/#get_ponderation_weights","text":"get_ponderation_weights() Method that returns the value of the linguistic quantifier (Q_function) for each value x Returns: ponderation_weights : ponderation of each client. [source]","title":"get_ponderation_weights"},{"location":"federated_aggregator/#clusterfedavgaggregator-class","text":"shfl.federated_aggregator.federated_aggregator.ClusterFedAvgAggregator(percentage=None) Implementation of Cluster Average Federated Aggregator. It adds another k-means to find the minimum distance of cluster centroids coming from each node. It implements Federated Aggregator","title":"ClusterFedAvgAggregator class"},{"location":"federated_government/","text":"[source] FederatedGovernment class shfl.federated_government.federated_government.FederatedGovernment(model_builder, federated_data, aggregator, model_params_access=None) Class used to represent the central class FederatedGoverment. Arguments: model_builder: Function that return a trainable model (see: Model ) federated_data: Federated data to use. (see: FederatedData ) aggregator: Federated aggregator function (see: Federated Aggregator ) model_param_access: Policy to access model's parameters, by default non-protected (see: DataAccessDefinition ) Properties: global_model : Return the global model. FederatedGovernment methods evaluate_global_model evaluate_global_model(data_test, label_test) Evaluation of the performance of the global model. Arguments: test_data : test dataset test_label : corresponding labels to test dataset deploy_central_model deploy_central_model() Deployment of the global learning model to each client (node) in the simulation. evaluate_clients evaluate_clients(data_test, label_test) Evaluation of local learning models over global test dataset. Arguments: test_data : test dataset test_label : corresponding labels to test dataset train_all_clients train_all_clients() Train all the clients aggregate_weights aggregate_weights() Aggregate weights from all data nodes in the server model run_rounds run_rounds(n, test_data, test_label) Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds test_data : Test data for evaluation between rounds test_label : Test label for evaluation between rounds [source] FederatedImagesClassifier class shfl.federated_government.federated_images_classifier.FederatedImagesClassifier(data_base_name_key, iid=True, num_nodes=20, percent=100) Class used to represent a high-level federated image classification (see: FederatedGoverment ). Arguments: data_base_name_key : key of the enumeration of valid data bases (see: ImagesDataBases ) iid : boolean which specifies if the distribution if IID (True) or non-IID (False) (True by default) num_nodes : number of clients. percent : percentage of the database to distribute among nodes. FederatedImagesClassifier methods run_rounds run_rounds(n=5) Overriding of the method of run_rounds of FederatedGoverment ). Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds (2 by default) model_builder model_builder() Create a Tensorflow Model for image classification. Returns: model : Instance of DeepLearningModel DeepLearningModel ). [source] Reshape class shfl.federated_government.federated_images_classifier.Reshape() Federated transformation to reshape the data [source] ImagesDataBases class shfl.federated_government.federated_images_classifier.ImagesDataBases() Enumeration of possible databases for image classification. [source] FederatedLinearRegression class shfl.federated_government.federated_linear_regression.FederatedLinearRegression(data_base_name_key, num_nodes=20, percent=100) Class used to represent a high-level federated linear regression (see: FederatedGoverment ). Arguments: data_base_name_key : key of the enumeration of valid data bases (see: LinearRegressionDatabases ) iid : boolean which specifies if the distribution if IID (True) or non-IID (False) (True by default) num_nodes : number of clients. percent : percentage of the database to distribute among nodes. FederatedLinearRegression methods run_rounds run_rounds(n=5) Overriding of the method of run_rounds of FederatedGoverment ). Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds (2 by default) model_builder model_builder() Create a Linear Regression Model. Returns: model : Linear Regression Model LinearRegressionModel ). [source] LinearRegressionDataBases class shfl.federated_government.federated_linear_regression.LinearRegressionDataBases() Enumeration of possible databases for clustering. [source] FederatedClustering class shfl.federated_government.federated_clustering.FederatedClustering(data_base_name_key, iid=True, num_nodes=20, percent=100) Class used to represent a high-level federated clustering using k-means (see: FederatedGoverment ). Arguments: data_base_name_key : key of the enumeration of valid data bases (see: ClusteringDataBases ) iid : boolean which specifies if the distribution if IID (True) or non-IID (False) (True by default) num_nodes : number of clients. percent : percentage of the database to distribute among nodes. FederatedClustering methods run_rounds run_rounds(n=5) Overriding of the method of run_rounds of FederatedGovernment ). Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds (2 by default) model_builder model_builder() Build a KMeans model with the class params. Returns: model : KMeans model. [source] ClusteringDataBases class shfl.federated_government.federated_clustering.ClusteringDataBases() Enumeration of possible databases for clustering. [source] IowaFederatedGovernment class shfl.federated_government.iowa_federated_government.IowaFederatedGovernment(model_builder, federated_data, model_params_access=None, dynamic=True, a=0, b=0.2, c=0.8, y_b=0.4, k=0.75) Class used to represent the IOWA Federated Government which implements FederatedGovernment Arguments: model_builder : Function that return a trainable model (see: Model ) federated_data : Federated data to use. (see: FederatedData ) aggregator : Federated aggregator function (see: Federated Aggregator ) model_param_access : Policy to access model's parameters, by default non-protected (see: DataAccessDefinition ) dynamic : boolean indicating if we use the dynamic or static version (default True) a : first argument of linguistic quantifier (default 0) b : second argument of linguistic quantifier (default 0.2) c : third argument of linguistic quantifier (default 0.8) y_b : fourth argument of linguistic quantifier (default 0.4) k : distance param of the dynamic version (default 3/4) IowaFederatedGovernment methods performance_clients performance_clients(data_val, label_val) Evaluation of local learning models over global test dataset. Arguments: val_data : validation dataset val_label : corresponding labels to validation dataset Returns: client_performance : Performance for each client.","title":"Federated Government"},{"location":"federated_government/#federatedgovernment-class","text":"shfl.federated_government.federated_government.FederatedGovernment(model_builder, federated_data, aggregator, model_params_access=None) Class used to represent the central class FederatedGoverment. Arguments: model_builder: Function that return a trainable model (see: Model ) federated_data: Federated data to use. (see: FederatedData ) aggregator: Federated aggregator function (see: Federated Aggregator ) model_param_access: Policy to access model's parameters, by default non-protected (see: DataAccessDefinition ) Properties: global_model : Return the global model.","title":"FederatedGovernment class"},{"location":"federated_government/#federatedgovernment-methods","text":"","title":"FederatedGovernment methods"},{"location":"federated_government/#evaluate_global_model","text":"evaluate_global_model(data_test, label_test) Evaluation of the performance of the global model. Arguments: test_data : test dataset test_label : corresponding labels to test dataset","title":"evaluate_global_model"},{"location":"federated_government/#deploy_central_model","text":"deploy_central_model() Deployment of the global learning model to each client (node) in the simulation.","title":"deploy_central_model"},{"location":"federated_government/#evaluate_clients","text":"evaluate_clients(data_test, label_test) Evaluation of local learning models over global test dataset. Arguments: test_data : test dataset test_label : corresponding labels to test dataset","title":"evaluate_clients"},{"location":"federated_government/#train_all_clients","text":"train_all_clients() Train all the clients","title":"train_all_clients"},{"location":"federated_government/#aggregate_weights","text":"aggregate_weights() Aggregate weights from all data nodes in the server model","title":"aggregate_weights"},{"location":"federated_government/#run_rounds","text":"run_rounds(n, test_data, test_label) Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds test_data : Test data for evaluation between rounds test_label : Test label for evaluation between rounds [source]","title":"run_rounds"},{"location":"federated_government/#federatedimagesclassifier-class","text":"shfl.federated_government.federated_images_classifier.FederatedImagesClassifier(data_base_name_key, iid=True, num_nodes=20, percent=100) Class used to represent a high-level federated image classification (see: FederatedGoverment ). Arguments: data_base_name_key : key of the enumeration of valid data bases (see: ImagesDataBases ) iid : boolean which specifies if the distribution if IID (True) or non-IID (False) (True by default) num_nodes : number of clients. percent : percentage of the database to distribute among nodes.","title":"FederatedImagesClassifier class"},{"location":"federated_government/#federatedimagesclassifier-methods","text":"","title":"FederatedImagesClassifier methods"},{"location":"federated_government/#run_rounds_1","text":"run_rounds(n=5) Overriding of the method of run_rounds of FederatedGoverment ). Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds (2 by default)","title":"run_rounds"},{"location":"federated_government/#model_builder","text":"model_builder() Create a Tensorflow Model for image classification. Returns: model : Instance of DeepLearningModel DeepLearningModel ). [source]","title":"model_builder"},{"location":"federated_government/#reshape-class","text":"shfl.federated_government.federated_images_classifier.Reshape() Federated transformation to reshape the data [source]","title":"Reshape class"},{"location":"federated_government/#imagesdatabases-class","text":"shfl.federated_government.federated_images_classifier.ImagesDataBases() Enumeration of possible databases for image classification. [source]","title":"ImagesDataBases class"},{"location":"federated_government/#federatedlinearregression-class","text":"shfl.federated_government.federated_linear_regression.FederatedLinearRegression(data_base_name_key, num_nodes=20, percent=100) Class used to represent a high-level federated linear regression (see: FederatedGoverment ). Arguments: data_base_name_key : key of the enumeration of valid data bases (see: LinearRegressionDatabases ) iid : boolean which specifies if the distribution if IID (True) or non-IID (False) (True by default) num_nodes : number of clients. percent : percentage of the database to distribute among nodes.","title":"FederatedLinearRegression class"},{"location":"federated_government/#federatedlinearregression-methods","text":"","title":"FederatedLinearRegression methods"},{"location":"federated_government/#run_rounds_2","text":"run_rounds(n=5) Overriding of the method of run_rounds of FederatedGoverment ). Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds (2 by default)","title":"run_rounds"},{"location":"federated_government/#model_builder_1","text":"model_builder() Create a Linear Regression Model. Returns: model : Linear Regression Model LinearRegressionModel ). [source]","title":"model_builder"},{"location":"federated_government/#linearregressiondatabases-class","text":"shfl.federated_government.federated_linear_regression.LinearRegressionDataBases() Enumeration of possible databases for clustering. [source]","title":"LinearRegressionDataBases class"},{"location":"federated_government/#federatedclustering-class","text":"shfl.federated_government.federated_clustering.FederatedClustering(data_base_name_key, iid=True, num_nodes=20, percent=100) Class used to represent a high-level federated clustering using k-means (see: FederatedGoverment ). Arguments: data_base_name_key : key of the enumeration of valid data bases (see: ClusteringDataBases ) iid : boolean which specifies if the distribution if IID (True) or non-IID (False) (True by default) num_nodes : number of clients. percent : percentage of the database to distribute among nodes.","title":"FederatedClustering class"},{"location":"federated_government/#federatedclustering-methods","text":"","title":"FederatedClustering methods"},{"location":"federated_government/#run_rounds_3","text":"run_rounds(n=5) Overriding of the method of run_rounds of FederatedGovernment ). Run one more round beginning in the actual state testing in test data and federated_local_test. Arguments: n : Number of rounds (2 by default)","title":"run_rounds"},{"location":"federated_government/#model_builder_2","text":"model_builder() Build a KMeans model with the class params. Returns: model : KMeans model. [source]","title":"model_builder"},{"location":"federated_government/#clusteringdatabases-class","text":"shfl.federated_government.federated_clustering.ClusteringDataBases() Enumeration of possible databases for clustering. [source]","title":"ClusteringDataBases class"},{"location":"federated_government/#iowafederatedgovernment-class","text":"shfl.federated_government.iowa_federated_government.IowaFederatedGovernment(model_builder, federated_data, model_params_access=None, dynamic=True, a=0, b=0.2, c=0.8, y_b=0.4, k=0.75) Class used to represent the IOWA Federated Government which implements FederatedGovernment Arguments: model_builder : Function that return a trainable model (see: Model ) federated_data : Federated data to use. (see: FederatedData ) aggregator : Federated aggregator function (see: Federated Aggregator ) model_param_access : Policy to access model's parameters, by default non-protected (see: DataAccessDefinition ) dynamic : boolean indicating if we use the dynamic or static version (default True) a : first argument of linguistic quantifier (default 0) b : second argument of linguistic quantifier (default 0.2) c : third argument of linguistic quantifier (default 0.8) y_b : fourth argument of linguistic quantifier (default 0.4) k : distance param of the dynamic version (default 3/4)","title":"IowaFederatedGovernment class"},{"location":"federated_government/#iowafederatedgovernment-methods","text":"","title":"IowaFederatedGovernment methods"},{"location":"federated_government/#performance_clients","text":"performance_clients(data_val, label_val) Evaluation of local learning models over global test dataset. Arguments: val_data : validation dataset val_label : corresponding labels to validation dataset Returns: client_performance : Performance for each client.","title":"performance_clients"},{"location":"getting-started/","text":"Getting started with Sherpa.ai Federated Learning Framework Sherpa.ai Federated Learning Framework is a python framework that provides an environment to develop research in the fields of private and distributed machine learning. The framework is designed with the goal of provide a set of tools allowing users to create and evaluate different aspects of this kind of algorithms with minimum code effort. The main big topics covered at the moment in the framework are federated learning and differential privacy. This techniques can be used together in order to increase the privacy of a federated learning algorithm. We have also develop a set of notebooks covering some of the most common use cases and explaining methodological aspects over different aspects. The full list of notebooks is here Even if are mainly interested in differential privacy a good point to start with Sherpa.ai Federated Learning Framework is to follow the following notebook where are explained the main concepts that are used all the time in the tutorials and in the documentation. The notebooks assume familiarity with python and some of the most popular libraries like numpy or keras/tensorflow. The documentation is divided following the different packages in the framework. In every section there is a brief introduction of the module with the purpose and some illustrative examples. In many cases the documentation links with a notebook illustrating the use of the different modules and classes. Package private contains most of the core elements of the framework that are used in almost every code that you will write with Sherpa.ai Federated Learning Framework. Package data_base introduces some datasets to work with. Package data_distribution provides some modules to distribute data among nodes. Package federated_aggregator has different algorithms to aggregate models. Package federated_government defines the communication and the kind of relationships between nodes. Package model provides a set of common models that you might want to use. Package differential_privacy introduces different differential privacy algorithms to protect data privacy when this must be shared.","title":"Getting started"},{"location":"getting-started/#getting-started-with-sherpaai-federated-learning-framework","text":"Sherpa.ai Federated Learning Framework is a python framework that provides an environment to develop research in the fields of private and distributed machine learning. The framework is designed with the goal of provide a set of tools allowing users to create and evaluate different aspects of this kind of algorithms with minimum code effort. The main big topics covered at the moment in the framework are federated learning and differential privacy. This techniques can be used together in order to increase the privacy of a federated learning algorithm. We have also develop a set of notebooks covering some of the most common use cases and explaining methodological aspects over different aspects. The full list of notebooks is here Even if are mainly interested in differential privacy a good point to start with Sherpa.ai Federated Learning Framework is to follow the following notebook where are explained the main concepts that are used all the time in the tutorials and in the documentation. The notebooks assume familiarity with python and some of the most popular libraries like numpy or keras/tensorflow. The documentation is divided following the different packages in the framework. In every section there is a brief introduction of the module with the purpose and some illustrative examples. In many cases the documentation links with a notebook illustrating the use of the different modules and classes. Package private contains most of the core elements of the framework that are used in almost every code that you will write with Sherpa.ai Federated Learning Framework. Package data_base introduces some datasets to work with. Package data_distribution provides some modules to distribute data among nodes. Package federated_aggregator has different algorithms to aggregate models. Package federated_government defines the communication and the kind of relationships between nodes. Package model provides a set of common models that you might want to use. Package differential_privacy introduces different differential privacy algorithms to protect data privacy when this must be shared.","title":"Getting started with Sherpa.ai Federated Learning Framework"},{"location":"install/","text":"Install We are on prerelease stage so at the moment the PyPI repository doesn't contain the package. In the future install should be as easy as write \"pip install shfl\". At the moment, the best option is install the package in editable mode, linking the package to the source code. From the main directory of the project you can write: pip install -e . The project documentation is mainly autogenerated from pydocs. If you want to generate the documentation just go to documentation directory and execute. python autogen.py The documentation is created using MkDocs . If you want to see the web version of the documentation you can serve after install it with in the documentation directory: mkdocs serve","title":"Install"},{"location":"install/#install","text":"We are on prerelease stage so at the moment the PyPI repository doesn't contain the package. In the future install should be as easy as write \"pip install shfl\". At the moment, the best option is install the package in editable mode, linking the package to the source code. From the main directory of the project you can write: pip install -e . The project documentation is mainly autogenerated from pydocs. If you want to generate the documentation just go to documentation directory and execute. python autogen.py The documentation is created using MkDocs . If you want to see the web version of the documentation you can serve after install it with in the documentation directory: mkdocs serve","title":"Install"},{"location":"learning_approach/","text":"Learning approach Defines the communication and the kind of relationships between nodes. {{autogenerated}}","title":"Learning approach"},{"location":"learning_approach/#learning-approach","text":"Defines the communication and the kind of relationships between nodes. {{autogenerated}}","title":"Learning approach"},{"location":"model/","text":"Model The goal of the framework is to provide a set of common models that you might want to use. We don't want to restrict or prioritize the use of neural networks over other, more classical machine learning methods. We know that a lot of the Machine Learning-based services and products in the industry are built over conventional Machine Learning techniques. In any case we are covering a huge range of models, including support for some popular deep learning frameworks. Of course, if you want to use your own model, the framework is easily extensible. TrainableModel is the abstract class that every model has to implement in order to interact with all the pieces of the framework. DeepLearningModel is a predefined model that takes a compiled keras or tensorflow model. If you want to define your own model only need to implement a class with methods defined by TrainableModel. [source] TrainableModel class shfl.model.model.TrainableModel() Interface of the models that can be trained. If you want to use a model that is not implemented in the framework you have to implement a class with this interface. TrainableModel methods train train(data, labels) Method that train the model Arguments: data : Data to train the model labels : Label for each train element predict predict(data) Predict labels for data Arguments: data : Data for predictions Returns: predictions : Matrix with predictions for data evaluate evaluate(data, labels) This method must return the performance in terms of different metrics of the prediction for those labels Arguments: data : Data to be evaluated labels : True values of data get_model_params get_model_params() Gets the params that define the model Returns: params : Parameters defining the model set_model_params set_model_params(params) Update the params that define the model Arguments: params : Parameters defining the model performance performance(data, labels) This method must return the performance of the prediction in terms of the most representative metric for those labels. Arguments: data : Data to be evaluated labels : True values of data [source] DeepLearningModel class shfl.model.deep_learning_model.DeepLearningModel(model, batch_size=None, epochs=1) This class offers support for Keras and tensorflow models. It implements TrainableModel Arguments: model : Compiled model, ready to train batch_size : batch_size to apply epochs : Number of epochs initialized : Indicates whether the model is initialized or not (default False) [source] LinearRegressionModel class shfl.model.linear_regression_model.LinearRegressionModel(n_features, n_targets=1) This class offers support for scikit-learn linear regression model. It implements TrainableModel Arguments: n_features : number of features (independent variables) n_targets : number of targets to predict (default is 1) [source] KMeansModel class shfl.model.kmeans_model.KMeansModel(n_clusters, n_features, init='k-means++', n_init=10) This class offers support for scikit-learn K-Means model. It implements TrainableModel Arguments: n_clusters : number of clusters. init : Method of initialization. {\u2018k-means++\u2019, \u2018random\u2019, ndarray}, default=\u2019k-means++\u2019. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. When \u2018random\u2019: choose n_clusters observations (rows) at random from data for the initial centroids. n_init : Number of time the k-means algorithm will be run with different centroid seeds (default 10). [source] LogisticRegressionModel class shfl.model.logistic_regression_model.LogisticRegressionModel(n_features, classes, model_inputs=None) This class offers support for scikit-learn logistic regression model. It implements TrainableModel Arguments: n_features : integer number of features (independent variables). classes : array of classes to predict. At least 2 classes must be provided. model_inputs : optional dictionary containing the model input parameters","title":"Model"},{"location":"model/#model","text":"The goal of the framework is to provide a set of common models that you might want to use. We don't want to restrict or prioritize the use of neural networks over other, more classical machine learning methods. We know that a lot of the Machine Learning-based services and products in the industry are built over conventional Machine Learning techniques. In any case we are covering a huge range of models, including support for some popular deep learning frameworks. Of course, if you want to use your own model, the framework is easily extensible. TrainableModel is the abstract class that every model has to implement in order to interact with all the pieces of the framework. DeepLearningModel is a predefined model that takes a compiled keras or tensorflow model. If you want to define your own model only need to implement a class with methods defined by TrainableModel. [source]","title":"Model"},{"location":"model/#trainablemodel-class","text":"shfl.model.model.TrainableModel() Interface of the models that can be trained. If you want to use a model that is not implemented in the framework you have to implement a class with this interface.","title":"TrainableModel class"},{"location":"model/#trainablemodel-methods","text":"","title":"TrainableModel methods"},{"location":"model/#train","text":"train(data, labels) Method that train the model Arguments: data : Data to train the model labels : Label for each train element","title":"train"},{"location":"model/#predict","text":"predict(data) Predict labels for data Arguments: data : Data for predictions Returns: predictions : Matrix with predictions for data","title":"predict"},{"location":"model/#evaluate","text":"evaluate(data, labels) This method must return the performance in terms of different metrics of the prediction for those labels Arguments: data : Data to be evaluated labels : True values of data","title":"evaluate"},{"location":"model/#get_model_params","text":"get_model_params() Gets the params that define the model Returns: params : Parameters defining the model","title":"get_model_params"},{"location":"model/#set_model_params","text":"set_model_params(params) Update the params that define the model Arguments: params : Parameters defining the model","title":"set_model_params"},{"location":"model/#performance","text":"performance(data, labels) This method must return the performance of the prediction in terms of the most representative metric for those labels. Arguments: data : Data to be evaluated labels : True values of data [source]","title":"performance"},{"location":"model/#deeplearningmodel-class","text":"shfl.model.deep_learning_model.DeepLearningModel(model, batch_size=None, epochs=1) This class offers support for Keras and tensorflow models. It implements TrainableModel Arguments: model : Compiled model, ready to train batch_size : batch_size to apply epochs : Number of epochs initialized : Indicates whether the model is initialized or not (default False) [source]","title":"DeepLearningModel class"},{"location":"model/#linearregressionmodel-class","text":"shfl.model.linear_regression_model.LinearRegressionModel(n_features, n_targets=1) This class offers support for scikit-learn linear regression model. It implements TrainableModel Arguments: n_features : number of features (independent variables) n_targets : number of targets to predict (default is 1) [source]","title":"LinearRegressionModel class"},{"location":"model/#kmeansmodel-class","text":"shfl.model.kmeans_model.KMeansModel(n_clusters, n_features, init='k-means++', n_init=10) This class offers support for scikit-learn K-Means model. It implements TrainableModel Arguments: n_clusters : number of clusters. init : Method of initialization. {\u2018k-means++\u2019, \u2018random\u2019, ndarray}, default=\u2019k-means++\u2019. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. When \u2018random\u2019: choose n_clusters observations (rows) at random from data for the initial centroids. n_init : Number of time the k-means algorithm will be run with different centroid seeds (default 10). [source]","title":"KMeansModel class"},{"location":"model/#logisticregressionmodel-class","text":"shfl.model.logistic_regression_model.LogisticRegressionModel(n_features, classes, model_inputs=None) This class offers support for scikit-learn logistic regression model. It implements TrainableModel Arguments: n_features : integer number of features (independent variables). classes : array of classes to predict. At least 2 classes must be provided. model_inputs : optional dictionary containing the model input parameters","title":"LogisticRegressionModel class"},{"location":"differential_privacy/composition/","text":"[source] ExceededPrivacyBudgetError shfl.differential_privacy.composition_dp.ExceededPrivacyBudgetError() This Exception is expected to be used when a certain privacy budget is exceed. When it is used, it means that the data cannot be accessed anymore Arguments: epsilon_delta : the privacy budget which has been surpassed [source] AdaptiveDifferentialPrivacy shfl.differential_privacy.composition_dp.AdaptiveDifferentialPrivacy(epsilon_delta, differentially_private_mechanism=None) It provides Adaptive Differential Privacy through Privacy Filters Arguments: epsilon_delta : Tuple or array of length 2 which contains the epsilon-delta privacy budget for this data differentially_private_mechanism : Optional. Default method that will be used to access data. If it is not set it's mandatory to pass it in every query. Properties: epsilon_delta : Return epsilon_delta value","title":"Composition"},{"location":"differential_privacy/composition/#exceededprivacybudgeterror","text":"shfl.differential_privacy.composition_dp.ExceededPrivacyBudgetError() This Exception is expected to be used when a certain privacy budget is exceed. When it is used, it means that the data cannot be accessed anymore Arguments: epsilon_delta : the privacy budget which has been surpassed [source]","title":"ExceededPrivacyBudgetError"},{"location":"differential_privacy/composition/#adaptivedifferentialprivacy","text":"shfl.differential_privacy.composition_dp.AdaptiveDifferentialPrivacy(epsilon_delta, differentially_private_mechanism=None) It provides Adaptive Differential Privacy through Privacy Filters Arguments: epsilon_delta : Tuple or array of length 2 which contains the epsilon-delta privacy budget for this data differentially_private_mechanism : Optional. Default method that will be used to access data. If it is not set it's mandatory to pass it in every query. Properties: epsilon_delta : Return epsilon_delta value","title":"AdaptiveDifferentialPrivacy"},{"location":"differential_privacy/mechanisms/","text":"[source] RandomizedResponseCoins shfl.differential_privacy.dp_mechanism.RandomizedResponseCoins(prob_head_first=0.5, prob_head_second=0.5) This class uses a simple mechanism to add randomness for binary data. Both the input and output are binary arrays or scalars. This algorithm is described by Cynthia Dwork and Aaron Roth in \"The algorithmic Foundations of Differential Privacy\". 1.- Flip a coin 2.- If tails, then respond truthfully. 3.- If heads, then flip a second coin and respond \"Yes\" if heads and \"No\" if tails. Input data must be binary, otherwise exception will be raised. This method is log(3)-differentially private Arguments prob_head_first : float in [0,1] representing probability to use a random response instead of true value. This is equivalent to prob_head of the first coin flip algorithm described by Dwork. prob_head_second : float in [0,1] representing probability of respond true when random answer is provided. Equivalent to prob_head in the second coin flip in the algorithm. Properties: epsilon_delta : Return epsilon_delta value References The algorithmic foundations of differential privacy [source] RandomizedResponseBinary shfl.differential_privacy.dp_mechanism.RandomizedResponseBinary(f0, f1, epsilon) Implements the most general binary randomized response algorithm. Both the input and output are binary arrays or scalars. The algorithm is defined through the conditional probabilities p00 = P( output=0 | input=0 ) = f0 p10 = P( output=1 | input=0 ) = 1 - f0 p11 = P( output=1 | input=1 ) = f1 p01 = P( output=0 | input=1 ) = 1 - f1 For f0=f1=0 or 1, the algorithm is not random. It is maximally random for f0=f1=1/2. This class contains, for special cases of f0, f1, the class RandomizedResponseCoins. This algorithm is epsilon-differentially private if epsilon >= log max{ p00/p01, p11/p10} = log max { f0/(1-f1), f1/(1-f0)} Input data must be binary, otherwise exception will be raised. Arguments f0 : float in [0,1] representing the probability of getting 0 when the input is 0 f1 : float in [0,1] representing the probability of getting 1 when the input is 1 Properties: epsilon_delta : Return epsilon_delta value References Using Randomized Response for Differential PrivacyPreserving Data Collection [source] LaplaceMechanism shfl.differential_privacy.dp_mechanism.LaplaceMechanism(sensitivity, epsilon, query=None) Implements the Laplace mechanism for differential privacy defined by Dwork in \"The algorithmic Foundations of Differential Privacy\". Notice that the Laplace mechanism is a randomization algorithm that depends on the l1 sensitivity, which can be regarded as a numeric query. One can show that this mechanism is epsilon-differentially private with epsilon = l1-sensitivity/b where b is a constant. In order to apply this mechanism for a particular value of epsilon, we need to compute the sensitivity, which might be hard to compute in practice. The framework provides a method to estimate the sensitivity of a query that maps the private data in a normed space (see: SensitivitySampler ) Arguments: sensitivity : float or array representing sensitivity of the applied query epsilon : float for the epsilon you want to apply query : Function to apply over private data (see: Query ). This parameter is optional and the identity function (see: IdentityFunction ) will be used if it is not provided. Properties: epsilon_delta : Return epsilon_delta value References The algorithmic foundations of differential privacy [source] GaussianMechanism shfl.differential_privacy.dp_mechanism.GaussianMechanism(sensitivity, epsilon_delta, query=None) Implements the Gaussian mechanism for differential privacy defined by Dwork in \"The algorithmic Foundations of Differential Privacy\". Notice that the Gaussian mechanism is a randomization algorithm that depends on the l2-sensitivity, which can be regarded as a numeric query. One can show that this mechanism is (epsilon, delta)-differentially private where noise is draw from a Gauss Distribution with zero mean and standard deviation equal to sqrt(2 * ln(1,25/delta)) * l2-sensivity / epsilon where epsilon is in the interval (0, 1) In order to apply this mechanism, we need to compute the l2-sensitivity, which might be hard to compute in practice. The framework provides a method to estimate the sensitivity of a query that maps the private data in a normed space (see: SensitivitySampler ) Arguments: sensitivity : float or array representing l2-sensitivity of the applied query epsilon : float for the epsilon you want to apply delta : float for the delta you want to apply query : Function to apply over private data (see: Query ). This parameter is optional and the identity function (see: IdentityFunction ) will be used if it is not provided. Properties: epsilon_delta : Return epsilon_delta value References The algorithmic foundations of differential privacy [source] ExponentialMechanism shfl.differential_privacy.dp_mechanism.ExponentialMechanism(u, r, delta_u, epsilon, size=1) Implements the exponential mechanism differential privacy defined by Dwork in \"The algorithmic Foundations of Differential Privacy\". Arguments: u : utility function with arguments x and r. It should be vectorized, so that for a particular database x, it returns as many values as given in r. r : array for the response space. delta_u : float for the sensitivity of the utility function. epsilon : float for the epsilon you want to apply. size : integer for the number of queries to perform at once. If not given it defaults to one. References The algorithmic foundations of differential privacy","title":"Mechanisms"},{"location":"differential_privacy/mechanisms/#randomizedresponsecoins","text":"shfl.differential_privacy.dp_mechanism.RandomizedResponseCoins(prob_head_first=0.5, prob_head_second=0.5) This class uses a simple mechanism to add randomness for binary data. Both the input and output are binary arrays or scalars. This algorithm is described by Cynthia Dwork and Aaron Roth in \"The algorithmic Foundations of Differential Privacy\". 1.- Flip a coin 2.- If tails, then respond truthfully. 3.- If heads, then flip a second coin and respond \"Yes\" if heads and \"No\" if tails. Input data must be binary, otherwise exception will be raised. This method is log(3)-differentially private Arguments prob_head_first : float in [0,1] representing probability to use a random response instead of true value. This is equivalent to prob_head of the first coin flip algorithm described by Dwork. prob_head_second : float in [0,1] representing probability of respond true when random answer is provided. Equivalent to prob_head in the second coin flip in the algorithm. Properties: epsilon_delta : Return epsilon_delta value References The algorithmic foundations of differential privacy [source]","title":"RandomizedResponseCoins"},{"location":"differential_privacy/mechanisms/#randomizedresponsebinary","text":"shfl.differential_privacy.dp_mechanism.RandomizedResponseBinary(f0, f1, epsilon) Implements the most general binary randomized response algorithm. Both the input and output are binary arrays or scalars. The algorithm is defined through the conditional probabilities p00 = P( output=0 | input=0 ) = f0 p10 = P( output=1 | input=0 ) = 1 - f0 p11 = P( output=1 | input=1 ) = f1 p01 = P( output=0 | input=1 ) = 1 - f1 For f0=f1=0 or 1, the algorithm is not random. It is maximally random for f0=f1=1/2. This class contains, for special cases of f0, f1, the class RandomizedResponseCoins. This algorithm is epsilon-differentially private if epsilon >= log max{ p00/p01, p11/p10} = log max { f0/(1-f1), f1/(1-f0)} Input data must be binary, otherwise exception will be raised. Arguments f0 : float in [0,1] representing the probability of getting 0 when the input is 0 f1 : float in [0,1] representing the probability of getting 1 when the input is 1 Properties: epsilon_delta : Return epsilon_delta value References Using Randomized Response for Differential PrivacyPreserving Data Collection [source]","title":"RandomizedResponseBinary"},{"location":"differential_privacy/mechanisms/#laplacemechanism","text":"shfl.differential_privacy.dp_mechanism.LaplaceMechanism(sensitivity, epsilon, query=None) Implements the Laplace mechanism for differential privacy defined by Dwork in \"The algorithmic Foundations of Differential Privacy\". Notice that the Laplace mechanism is a randomization algorithm that depends on the l1 sensitivity, which can be regarded as a numeric query. One can show that this mechanism is epsilon-differentially private with epsilon = l1-sensitivity/b where b is a constant. In order to apply this mechanism for a particular value of epsilon, we need to compute the sensitivity, which might be hard to compute in practice. The framework provides a method to estimate the sensitivity of a query that maps the private data in a normed space (see: SensitivitySampler ) Arguments: sensitivity : float or array representing sensitivity of the applied query epsilon : float for the epsilon you want to apply query : Function to apply over private data (see: Query ). This parameter is optional and the identity function (see: IdentityFunction ) will be used if it is not provided. Properties: epsilon_delta : Return epsilon_delta value References The algorithmic foundations of differential privacy [source]","title":"LaplaceMechanism"},{"location":"differential_privacy/mechanisms/#gaussianmechanism","text":"shfl.differential_privacy.dp_mechanism.GaussianMechanism(sensitivity, epsilon_delta, query=None) Implements the Gaussian mechanism for differential privacy defined by Dwork in \"The algorithmic Foundations of Differential Privacy\". Notice that the Gaussian mechanism is a randomization algorithm that depends on the l2-sensitivity, which can be regarded as a numeric query. One can show that this mechanism is (epsilon, delta)-differentially private where noise is draw from a Gauss Distribution with zero mean and standard deviation equal to sqrt(2 * ln(1,25/delta)) * l2-sensivity / epsilon where epsilon is in the interval (0, 1) In order to apply this mechanism, we need to compute the l2-sensitivity, which might be hard to compute in practice. The framework provides a method to estimate the sensitivity of a query that maps the private data in a normed space (see: SensitivitySampler ) Arguments: sensitivity : float or array representing l2-sensitivity of the applied query epsilon : float for the epsilon you want to apply delta : float for the delta you want to apply query : Function to apply over private data (see: Query ). This parameter is optional and the identity function (see: IdentityFunction ) will be used if it is not provided. Properties: epsilon_delta : Return epsilon_delta value References The algorithmic foundations of differential privacy [source]","title":"GaussianMechanism"},{"location":"differential_privacy/mechanisms/#exponentialmechanism","text":"shfl.differential_privacy.dp_mechanism.ExponentialMechanism(u, r, delta_u, epsilon, size=1) Implements the exponential mechanism differential privacy defined by Dwork in \"The algorithmic Foundations of Differential Privacy\". Arguments: u : utility function with arguments x and r. It should be vectorized, so that for a particular database x, it returns as many values as given in r. r : array for the response space. delta_u : float for the sensitivity of the utility function. epsilon : float for the epsilon you want to apply. size : integer for the number of queries to perform at once. If not given it defaults to one. References The algorithmic foundations of differential privacy","title":"ExponentialMechanism"},{"location":"differential_privacy/norm/","text":"[source] SensitivityNorm class shfl.differential_privacy.norm.SensitivityNorm() This class defines the interface that must be implemented to compute the sensitivity norm between two values in a normed space. SensitivityNorm methods compute compute(x_1, x_2) The compute method receives the result of apply a certain function over private data and returns the norm of the responses Arguments: x_1 : array response from a concrete query over database 1 x_2 : array response from the same query over database 2 [source] L1SensitivityNorm class shfl.differential_privacy.norm.L1SensitivityNorm() Implements the L1 norm of the difference between x_1 and x_2 [source] L2SensitivityNorm class shfl.differential_privacy.norm.L2SensitivityNorm() Implements the L2 norm of the difference between x_1 and x_2","title":"Norm"},{"location":"differential_privacy/norm/#sensitivitynorm-class","text":"shfl.differential_privacy.norm.SensitivityNorm() This class defines the interface that must be implemented to compute the sensitivity norm between two values in a normed space.","title":"SensitivityNorm class"},{"location":"differential_privacy/norm/#sensitivitynorm-methods","text":"","title":"SensitivityNorm methods"},{"location":"differential_privacy/norm/#compute","text":"compute(x_1, x_2) The compute method receives the result of apply a certain function over private data and returns the norm of the responses Arguments: x_1 : array response from a concrete query over database 1 x_2 : array response from the same query over database 2 [source]","title":"compute"},{"location":"differential_privacy/norm/#l1sensitivitynorm-class","text":"shfl.differential_privacy.norm.L1SensitivityNorm() Implements the L1 norm of the difference between x_1 and x_2 [source]","title":"L1SensitivityNorm class"},{"location":"differential_privacy/norm/#l2sensitivitynorm-class","text":"shfl.differential_privacy.norm.L2SensitivityNorm() Implements the L2 norm of the difference between x_1 and x_2","title":"L2SensitivityNorm class"},{"location":"differential_privacy/overview/","text":"Differential Privacy This package contains all the elements related to differential privacy. The framework implements the different elements allowing use differential privacy isolated or combined with federated learning. Mechanisms contains the main algorithms to apply dp. Sensitivity Sampler provides a method to estimate epsilon for the cases where the privacy algorithm analysis is extremely hard. Composition methods provide some useful methods to deal with composition. Sampling methods add the option of reduce the amount of privacy consumed with a query.","title":"Differential Privacy"},{"location":"differential_privacy/overview/#differential-privacy","text":"This package contains all the elements related to differential privacy. The framework implements the different elements allowing use differential privacy isolated or combined with federated learning. Mechanisms contains the main algorithms to apply dp. Sensitivity Sampler provides a method to estimate epsilon for the cases where the privacy algorithm analysis is extremely hard. Composition methods provide some useful methods to deal with composition. Sampling methods add the option of reduce the amount of privacy consumed with a query.","title":"Differential Privacy"},{"location":"differential_privacy/probability_distribution/","text":"[source] ProbabilityDistribution class shfl.differential_privacy.probability_distribution.ProbabilityDistribution() Class representing the interface for a probability distribution ProbabilityDistribution methods sample sample(size) This method must return an array with length \"size\", sampling the distribution Arguments: size : Size of the sampling [source] NormalDistribution class shfl.differential_privacy.probability_distribution.NormalDistribution(mean, std) Implements Normal Distribution Arguments: mean : Mean of the normal distribution. std : Standard deviation of the normal distribution [source] GaussianMixture class shfl.differential_privacy.probability_distribution.GaussianMixture(params, weights) Implements the combination of Normal Distributions Arguments: params : Array of arrays with mean and std for every gaussian distribution. weights : Array of weights for every distribution with sum 1. Example: # Parameters for two Gaussian mu_M = 178 mu_F = 162 sigma_M = 7 sigma_F = 7 # Parameters norm_params = np.array([[mu_M, sigma_M], [mu_F, sigma_F]]) weights = np.ones(2) / 2.0 # Creating combination of gaussian distribution = GaussianMixture(norm_params, weights)","title":"Probability Distribution"},{"location":"differential_privacy/probability_distribution/#probabilitydistribution-class","text":"shfl.differential_privacy.probability_distribution.ProbabilityDistribution() Class representing the interface for a probability distribution","title":"ProbabilityDistribution class"},{"location":"differential_privacy/probability_distribution/#probabilitydistribution-methods","text":"","title":"ProbabilityDistribution methods"},{"location":"differential_privacy/probability_distribution/#sample","text":"sample(size) This method must return an array with length \"size\", sampling the distribution Arguments: size : Size of the sampling [source]","title":"sample"},{"location":"differential_privacy/probability_distribution/#normaldistribution-class","text":"shfl.differential_privacy.probability_distribution.NormalDistribution(mean, std) Implements Normal Distribution Arguments: mean : Mean of the normal distribution. std : Standard deviation of the normal distribution [source]","title":"NormalDistribution class"},{"location":"differential_privacy/probability_distribution/#gaussianmixture-class","text":"shfl.differential_privacy.probability_distribution.GaussianMixture(params, weights) Implements the combination of Normal Distributions Arguments: params : Array of arrays with mean and std for every gaussian distribution. weights : Array of weights for every distribution with sum 1. Example: # Parameters for two Gaussian mu_M = 178 mu_F = 162 sigma_M = 7 sigma_F = 7 # Parameters norm_params = np.array([[mu_M, sigma_M], [mu_F, sigma_F]]) weights = np.ones(2) / 2.0 # Creating combination of gaussian distribution = GaussianMixture(norm_params, weights)","title":"GaussianMixture class"},{"location":"differential_privacy/sampling/","text":"[source] Sampler class shfl.differential_privacy.dp_sampling.Sampler(dp_mechanism) This class implements sampling methods which helps to reduce the epsilon-delta budget spent by a dp-mechanism Arguments: sample_size : size of the sample to be taken Sampler methods epsilon_delta_reduction epsilon_delta_reduction(epsilon_delta) It receives epsilon_delta parameters from a dp-mechanism and computes the new hopefully reduced epsilon_delta Arguments: epsilon_delta : privacy budget provided by a dp-mechanism Returns: new_epsilon_delta : new hopefully reduced epsilon_delta sample sample(data) It receives some data and returns a sample of it Arguments: data : Raw data that are going to be sampled Returns: sampled_data : sample of size self._sample_size [source] SampleWithoutReplacement class shfl.differential_privacy.dp_sampling.SampleWithoutReplacement(dp_mechanism, sample_size, data_size) It implements the sample with replacement technique (Theorem 9 from the reference) which reduces the epsilon-delta bugdet spent specified. Note that it only can sample the first dimension of a ndarray. Arguments: sample_size : one dimentional size of the sample data_size : shape of the input data References: Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences prod shfl.differential_privacy.dp_sampling.prod(iterable) This is a multiplicational equivalent of python sum function check_sample_size shfl.differential_privacy.dp_sampling.check_sample_size(sample_size, data_size) This method ensures that the sample simple is smaller than the first dimension of the data_size Arguments: sample_size : one dimentional size of the sample data_size : shape of the given data, a tuple is expected","title":"Sampling"},{"location":"differential_privacy/sampling/#sampler-class","text":"shfl.differential_privacy.dp_sampling.Sampler(dp_mechanism) This class implements sampling methods which helps to reduce the epsilon-delta budget spent by a dp-mechanism Arguments: sample_size : size of the sample to be taken","title":"Sampler class"},{"location":"differential_privacy/sampling/#sampler-methods","text":"","title":"Sampler methods"},{"location":"differential_privacy/sampling/#epsilon_delta_reduction","text":"epsilon_delta_reduction(epsilon_delta) It receives epsilon_delta parameters from a dp-mechanism and computes the new hopefully reduced epsilon_delta Arguments: epsilon_delta : privacy budget provided by a dp-mechanism Returns: new_epsilon_delta : new hopefully reduced epsilon_delta","title":"epsilon_delta_reduction"},{"location":"differential_privacy/sampling/#sample","text":"sample(data) It receives some data and returns a sample of it Arguments: data : Raw data that are going to be sampled Returns: sampled_data : sample of size self._sample_size [source]","title":"sample"},{"location":"differential_privacy/sampling/#samplewithoutreplacement-class","text":"shfl.differential_privacy.dp_sampling.SampleWithoutReplacement(dp_mechanism, sample_size, data_size) It implements the sample with replacement technique (Theorem 9 from the reference) which reduces the epsilon-delta bugdet spent specified. Note that it only can sample the first dimension of a ndarray. Arguments: sample_size : one dimentional size of the sample data_size : shape of the input data References: Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences","title":"SampleWithoutReplacement class"},{"location":"differential_privacy/sampling/#prod","text":"shfl.differential_privacy.dp_sampling.prod(iterable) This is a multiplicational equivalent of python sum function","title":"prod"},{"location":"differential_privacy/sampling/#check_sample_size","text":"shfl.differential_privacy.dp_sampling.check_sample_size(sample_size, data_size) This method ensures that the sample simple is smaller than the first dimension of the data_size Arguments: sample_size : one dimentional size of the sample data_size : shape of the given data, a tuple is expected","title":"check_sample_size"},{"location":"differential_privacy/sensitivity_sampler/","text":"[source] SensitivitySampler shfl.differential_privacy.sensitivity_sampler.SensitivitySampler() This class implements the algorithm described in the article Benjamin I. P. Rubinstein and Francesco Ald\u00e0 \"Pain-Free Random Differential Privacy with Sensitivity Sampling\", accepted into the 34th International Conference on Machine Learning (ICML'2017), May 2017. It provides a method to estimate the sensitivity of a generic query using a concrete sensitivity norm. References Pain-Free Random Differential Privacy with Sensitivity Sampling sample_sensitivity sample_sensitivity(query, sensitivity_norm, oracle, n, m=None, gamma=None) This method calculates the parameters to sample the oracle and estimates the sensitivity. One of m or gamma must be provided. Arguments: query : Function to apply over private data (see: Query ) sensitivity_norm : Function to compute the sensitivity norm (see: Norm ) oracle : ProbabilityDistribution to sample. n : int for size of private data m : int for size of sampling gamma : float for privacy confidence level Returns: sensitivity : Calculated sensitivity value by the sampler mean : Mean sensitivity from all samples.","title":"Sensitivity Sampler"},{"location":"differential_privacy/sensitivity_sampler/#sensitivitysampler","text":"shfl.differential_privacy.sensitivity_sampler.SensitivitySampler() This class implements the algorithm described in the article Benjamin I. P. Rubinstein and Francesco Ald\u00e0 \"Pain-Free Random Differential Privacy with Sensitivity Sampling\", accepted into the 34th International Conference on Machine Learning (ICML'2017), May 2017. It provides a method to estimate the sensitivity of a generic query using a concrete sensitivity norm. References Pain-Free Random Differential Privacy with Sensitivity Sampling","title":"SensitivitySampler"},{"location":"differential_privacy/sensitivity_sampler/#sample_sensitivity","text":"sample_sensitivity(query, sensitivity_norm, oracle, n, m=None, gamma=None) This method calculates the parameters to sample the oracle and estimates the sensitivity. One of m or gamma must be provided. Arguments: query : Function to apply over private data (see: Query ) sensitivity_norm : Function to compute the sensitivity norm (see: Norm ) oracle : ProbabilityDistribution to sample. n : int for size of private data m : int for size of sampling gamma : float for privacy confidence level Returns: sensitivity : Calculated sensitivity value by the sampler mean : Mean sensitivity from all samples.","title":"sample_sensitivity"},{"location":"private/data/","text":"[source] LabeledData shfl.private.data.LabeledData(data, label) Class to represent labeled data Arguments: data : Features representing a data sample label : Label for this sample Properties: data : getter and setter for data label : getter and setter for the data label [source] DataAccessDefinition class shfl.private.data.DataAccessDefinition() Interface that must be implemented in order to define how to access the private data. DataAccessDefinition methods apply apply(data) Every implementation needs to implement this method defining how data will be returned. Arguments: data : Raw data that are going to be accessed Returns: result_data : Result data, function of argument data [source] DPDataAccessDefinition class shfl.private.data.DPDataAccessDefinition() Interface that must be implemented in order to define how to access differentially private data. Moreover, it provides some tools to ensure a proper implementation of Differential Privacy. [source] UnprotectedAccess class shfl.private.data.UnprotectedAccess() This class implements access to data without restrictions, plain data will be returned.","title":"Data"},{"location":"private/data/#labeleddata","text":"shfl.private.data.LabeledData(data, label) Class to represent labeled data Arguments: data : Features representing a data sample label : Label for this sample Properties: data : getter and setter for data label : getter and setter for the data label [source]","title":"LabeledData"},{"location":"private/data/#dataaccessdefinition-class","text":"shfl.private.data.DataAccessDefinition() Interface that must be implemented in order to define how to access the private data.","title":"DataAccessDefinition class"},{"location":"private/data/#dataaccessdefinition-methods","text":"","title":"DataAccessDefinition methods"},{"location":"private/data/#apply","text":"apply(data) Every implementation needs to implement this method defining how data will be returned. Arguments: data : Raw data that are going to be accessed Returns: result_data : Result data, function of argument data [source]","title":"apply"},{"location":"private/data/#dpdataaccessdefinition-class","text":"shfl.private.data.DPDataAccessDefinition() Interface that must be implemented in order to define how to access differentially private data. Moreover, it provides some tools to ensure a proper implementation of Differential Privacy. [source]","title":"DPDataAccessDefinition class"},{"location":"private/data/#unprotectedaccess-class","text":"shfl.private.data.UnprotectedAccess() This class implements access to data without restrictions, plain data will be returned.","title":"UnprotectedAccess class"},{"location":"private/data_node/","text":"[source] DataNode shfl.private.node.DataNode() This class represents an independent data node. A DataNode has its own private data and provides methods to initialize this data and access to it. The access to private data needs to be configured with an access policy before query it or an exception will be raised. A method to transform private data is also provided. This is a mechanism that allows data preprocessing or related task over data. A model (see: Model ) can be deployed in the DataNode and use private data in order to learn. It is assumed that a model is represented by its parameters and the access to these parameters must be also configured before queries. Properties: model : access to the model private_data : access to train data private_data_test : access to test data set_private_data set_private_data(name, data) Creates copy of data in private memory using name as key. If there is a previous value with this key the data will be overridden. Arguments: name : String with the key identifier for the data data : Data to be stored in the private memory of the DataNode set_private_test_data set_private_test_data(name, data) Creates copy of test data in private memory using name as key. If there is a previous value with this key the data will be override. Arguments: name : String with the key identifier for the data data : Data to be stored in the private memory of the DataNode configure_data_access configure_data_access(name, data_access_definition) Adds a DataAccessDefinition for some concrete private data. Arguments: name : Identifier for the data that will be configured data_access_definition : Policy to access data (see: DataAccessDefinition ) configure_model_params_access configure_model_params_access(data_access_definition) Adds a DataAccessDefinition for model parameters. Arguments: data_access_definition : Policy to access parameters (see: DataAccessDefinition ) apply_data_transformation apply_data_transformation(private_property, federated_transformation) Executes FederatedTransformation (see: Federated Operation ) over private data. Arguments: private_property : Identifier for the data that will be transformed federated_transformation : Operation to execute (see: Federated Operation ) query query(private_property) Queries private data previously configured. If the access didn't configured this method will raise exception Arguments: private_property : String with the key identifier for the data query_model_params query_model_params() Queries model parameters. By default the parameters access is unprotected but access definition can be changed set_model_params set_model_params(model_params) Sets the model to use in the node Arguments: model_params : Parameters to set in the model train_model train_model(training_data_key) Train the model that has been previously set in the data node Arguments: training_data_key : String identifying the private data to use for this model. This key must contain LabeledData (see: LabeledData ) predict predict(data) Uses the model to predict new data Arguments: data : Data to predict Returns: predictions : array with predictions for data argument. evaluate evaluate(data, labels) Evaluates the performance of the model Arguments: data : Data to predict labels : True values of data Returns: metrics : array with metrics values for predictions for data argument. performance performance(data, labels) Evaluates the performance of the model in terms of the most representative metric. Arguments: data : Data to predict labels : True values of data Returns: metric : return the main metric value local_evaluate local_evaluate(data_key) Evaluation of local models on local data test Arguments: data_key : key of the private data of the client","title":"DataNode"},{"location":"private/data_node/#datanode","text":"shfl.private.node.DataNode() This class represents an independent data node. A DataNode has its own private data and provides methods to initialize this data and access to it. The access to private data needs to be configured with an access policy before query it or an exception will be raised. A method to transform private data is also provided. This is a mechanism that allows data preprocessing or related task over data. A model (see: Model ) can be deployed in the DataNode and use private data in order to learn. It is assumed that a model is represented by its parameters and the access to these parameters must be also configured before queries. Properties: model : access to the model private_data : access to train data private_data_test : access to test data","title":"DataNode"},{"location":"private/data_node/#set_private_data","text":"set_private_data(name, data) Creates copy of data in private memory using name as key. If there is a previous value with this key the data will be overridden. Arguments: name : String with the key identifier for the data data : Data to be stored in the private memory of the DataNode","title":"set_private_data"},{"location":"private/data_node/#set_private_test_data","text":"set_private_test_data(name, data) Creates copy of test data in private memory using name as key. If there is a previous value with this key the data will be override. Arguments: name : String with the key identifier for the data data : Data to be stored in the private memory of the DataNode","title":"set_private_test_data"},{"location":"private/data_node/#configure_data_access","text":"configure_data_access(name, data_access_definition) Adds a DataAccessDefinition for some concrete private data. Arguments: name : Identifier for the data that will be configured data_access_definition : Policy to access data (see: DataAccessDefinition )","title":"configure_data_access"},{"location":"private/data_node/#configure_model_params_access","text":"configure_model_params_access(data_access_definition) Adds a DataAccessDefinition for model parameters. Arguments: data_access_definition : Policy to access parameters (see: DataAccessDefinition )","title":"configure_model_params_access"},{"location":"private/data_node/#apply_data_transformation","text":"apply_data_transformation(private_property, federated_transformation) Executes FederatedTransformation (see: Federated Operation ) over private data. Arguments: private_property : Identifier for the data that will be transformed federated_transformation : Operation to execute (see: Federated Operation )","title":"apply_data_transformation"},{"location":"private/data_node/#query","text":"query(private_property) Queries private data previously configured. If the access didn't configured this method will raise exception Arguments: private_property : String with the key identifier for the data","title":"query"},{"location":"private/data_node/#query_model_params","text":"query_model_params() Queries model parameters. By default the parameters access is unprotected but access definition can be changed","title":"query_model_params"},{"location":"private/data_node/#set_model_params","text":"set_model_params(model_params) Sets the model to use in the node Arguments: model_params : Parameters to set in the model","title":"set_model_params"},{"location":"private/data_node/#train_model","text":"train_model(training_data_key) Train the model that has been previously set in the data node Arguments: training_data_key : String identifying the private data to use for this model. This key must contain LabeledData (see: LabeledData )","title":"train_model"},{"location":"private/data_node/#predict","text":"predict(data) Uses the model to predict new data Arguments: data : Data to predict Returns: predictions : array with predictions for data argument.","title":"predict"},{"location":"private/data_node/#evaluate","text":"evaluate(data, labels) Evaluates the performance of the model Arguments: data : Data to predict labels : True values of data Returns: metrics : array with metrics values for predictions for data argument.","title":"evaluate"},{"location":"private/data_node/#performance","text":"performance(data, labels) Evaluates the performance of the model in terms of the most representative metric. Arguments: data : Data to predict labels : True values of data Returns: metric : return the main metric value","title":"performance"},{"location":"private/data_node/#local_evaluate","text":"local_evaluate(data_key) Evaluation of local models on local data test Arguments: data_key : key of the private data of the client","title":"local_evaluate"},{"location":"private/federated_attack/","text":"[source] FederatedDataAttack class shfl.private.federated_attack.FederatedDataAttack() Interface defining method to apply an FederatedAttack over FederatedData FederatedDataAttack methods apply_attack apply_attack(data) This method receives federated data to be modified and performs the required modifications (federated_attack) over it simulating the adversarial attack. Arguments: federated_data : The data of nodes that we attack [source] FederatedPoisoningDataAttack class shfl.private.federated_attack.FederatedPoisoningDataAttack(percentage) Class representing poisoning data attack simulation. This simulation consists on shuffling the labels of some nodes. For that purpose, it uses class ShuffleNode . This class implements interface FederatedDataAttack . Arguments: percentage : percentage of nodes that are adversarial ones Properties: adversaries : Returns adversaries value [source] ShuffleNode class shfl.private.federated_attack.ShuffleNode() Implementation of Federated Transformation for shuffling labels of labeled data in order to implement data poisoning attack. This class implements interface FederatedTransformation .","title":"Federated Attack"},{"location":"private/federated_attack/#federateddataattack-class","text":"shfl.private.federated_attack.FederatedDataAttack() Interface defining method to apply an FederatedAttack over FederatedData","title":"FederatedDataAttack class"},{"location":"private/federated_attack/#federateddataattack-methods","text":"","title":"FederatedDataAttack methods"},{"location":"private/federated_attack/#apply_attack","text":"apply_attack(data) This method receives federated data to be modified and performs the required modifications (federated_attack) over it simulating the adversarial attack. Arguments: federated_data : The data of nodes that we attack [source]","title":"apply_attack"},{"location":"private/federated_attack/#federatedpoisoningdataattack-class","text":"shfl.private.federated_attack.FederatedPoisoningDataAttack(percentage) Class representing poisoning data attack simulation. This simulation consists on shuffling the labels of some nodes. For that purpose, it uses class ShuffleNode . This class implements interface FederatedDataAttack . Arguments: percentage : percentage of nodes that are adversarial ones Properties: adversaries : Returns adversaries value [source]","title":"FederatedPoisoningDataAttack class"},{"location":"private/federated_attack/#shufflenode-class","text":"shfl.private.federated_attack.ShuffleNode() Implementation of Federated Transformation for shuffling labels of labeled data in order to implement data poisoning attack. This class implements interface FederatedTransformation .","title":"ShuffleNode class"},{"location":"private/federated_operation/","text":"[source] FederatedData class shfl.private.federated_operation.FederatedData() Class representing data across different data nodes. This object is iterable over different data nodes. FederatedData methods add_data_node add_data_node(data) This method adds a new node containing data to the federated data Arguments: data : Data to add to this node num_nodes num_nodes() Returns: num_nodes : The number of nodes in this federated data. configure_data_access configure_data_access(data_access_definition) Creates the same policy to access data over all the data nodes Arguments: data_access_definition : (see: DataAccessDefinition ) query query() Queries over every node and returns the answer of every node in a list Returns: answer : List containing responses for every node [source] FederatedDataNode class shfl.private.federated_operation.FederatedDataNode(federated_data_identifier) This class represents a DataNode in a FederatedData. Extends DataNode allowing calls to methods without explicit private data identifier, assuming access to the federated data. It supports Adaptive Differential Privacy through Privacy Filters Arguments: federated_data_identifier : identifier to use in private data When you iterate over FederatedData the kind of DataNode that you obtain is a FederatedDataNode. Example: # Definition of federated data from dataset database = shfl.data_base.Emnist() iid_distribution = shfl.data_distribution.IidDataDistribution(database) federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=20, percent=10) # Data access definition and query node 0 federated_data.configure_data_access(UnprotectedAccess()) federated_data[0].query() FederatedDataNode methods configure_data_access configure_data_access(data_access_definition) Adds a DataAccessDefinition for some concrete private data. Arguments: data_access_definition : Policy to access data (see: DataAccessDefinition ) set_private_data set_private_data(data) Creates copy of data in private memory using name as key. If there is a previous value with this key the data will be overridden. Arguments: data : Data to be stored in the private memory of the DataNode set_private_test_data set_private_test_data(data) Creates copy of test data in private memory using name as key. If there is a previous value with this key the data will be override. Arguments: data : Data to be stored in the private memory of the DataNode train_model train_model() Train the model that has been previously set in the data node apply_data_transformation apply_data_transformation(federated_transformation) Executes FederatedTransformation (see: Federated Operation ) over private data. Arguments: federated_transformation : Operation to execute (see: Federated Operation ) split_train_test split_train_test(test_split=0.2) Splits private_data in train and test sets Arguments: test_split : percentage of test split [source] FederatedTransformation class shfl.private.federated_operation.FederatedTransformation() Interface defining the method for applying an operation over FederatedData FederatedTransformation methods apply apply(data) This method receives data to be modified and performs the required modifications over it. Arguments: data : The object that has to be modified [source] Normalize class shfl.private.federated_operation.Normalize(mean, std) Normalization class of federated data FederatedData . It implements FederatedTransformation . Arguments: mean : mean used for normalization. std : std used for normalization. federate_array shfl.private.federated_operation.federate_array(array, num_data_nodes) Creates FederatedData from an indexable array. The array will be divided using the first dimension. It supports Adaptive Differential Privacy through Privacy Filters Arguments: array : Indexable array with any number of dimensions num_data_nodes : Number of nodes to use Returns: federated_array : FederatedData with an array of size len(array)/num_data_nodes in every node apply_federated_transformation shfl.private.federated_operation.apply_federated_transformation(federated_data, federated_transformation) Applies the federated transformation over this federated data. Original federated data will be modified. Arguments: federated_data : FederatedData to use in the transformation federated_transformation : FederatedTransformation that will be applied over this data split_train_test shfl.private.federated_operation.split_train_test(federated_data, test_split=0.2) Splits all data nodes in train and test sets Arguments: federated_data : FederatedData test_split : percentage of test split","title":"Federated Operation"},{"location":"private/federated_operation/#federateddata-class","text":"shfl.private.federated_operation.FederatedData() Class representing data across different data nodes. This object is iterable over different data nodes.","title":"FederatedData class"},{"location":"private/federated_operation/#federateddata-methods","text":"","title":"FederatedData methods"},{"location":"private/federated_operation/#add_data_node","text":"add_data_node(data) This method adds a new node containing data to the federated data Arguments: data : Data to add to this node","title":"add_data_node"},{"location":"private/federated_operation/#num_nodes","text":"num_nodes() Returns: num_nodes : The number of nodes in this federated data.","title":"num_nodes"},{"location":"private/federated_operation/#configure_data_access","text":"configure_data_access(data_access_definition) Creates the same policy to access data over all the data nodes Arguments: data_access_definition : (see: DataAccessDefinition )","title":"configure_data_access"},{"location":"private/federated_operation/#query","text":"query() Queries over every node and returns the answer of every node in a list Returns: answer : List containing responses for every node [source]","title":"query"},{"location":"private/federated_operation/#federateddatanode-class","text":"shfl.private.federated_operation.FederatedDataNode(federated_data_identifier) This class represents a DataNode in a FederatedData. Extends DataNode allowing calls to methods without explicit private data identifier, assuming access to the federated data. It supports Adaptive Differential Privacy through Privacy Filters Arguments: federated_data_identifier : identifier to use in private data When you iterate over FederatedData the kind of DataNode that you obtain is a FederatedDataNode. Example: # Definition of federated data from dataset database = shfl.data_base.Emnist() iid_distribution = shfl.data_distribution.IidDataDistribution(database) federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=20, percent=10) # Data access definition and query node 0 federated_data.configure_data_access(UnprotectedAccess()) federated_data[0].query()","title":"FederatedDataNode class"},{"location":"private/federated_operation/#federateddatanode-methods","text":"","title":"FederatedDataNode methods"},{"location":"private/federated_operation/#configure_data_access_1","text":"configure_data_access(data_access_definition) Adds a DataAccessDefinition for some concrete private data. Arguments: data_access_definition : Policy to access data (see: DataAccessDefinition )","title":"configure_data_access"},{"location":"private/federated_operation/#set_private_data","text":"set_private_data(data) Creates copy of data in private memory using name as key. If there is a previous value with this key the data will be overridden. Arguments: data : Data to be stored in the private memory of the DataNode","title":"set_private_data"},{"location":"private/federated_operation/#set_private_test_data","text":"set_private_test_data(data) Creates copy of test data in private memory using name as key. If there is a previous value with this key the data will be override. Arguments: data : Data to be stored in the private memory of the DataNode","title":"set_private_test_data"},{"location":"private/federated_operation/#train_model","text":"train_model() Train the model that has been previously set in the data node","title":"train_model"},{"location":"private/federated_operation/#apply_data_transformation","text":"apply_data_transformation(federated_transformation) Executes FederatedTransformation (see: Federated Operation ) over private data. Arguments: federated_transformation : Operation to execute (see: Federated Operation )","title":"apply_data_transformation"},{"location":"private/federated_operation/#split_train_test","text":"split_train_test(test_split=0.2) Splits private_data in train and test sets Arguments: test_split : percentage of test split [source]","title":"split_train_test"},{"location":"private/federated_operation/#federatedtransformation-class","text":"shfl.private.federated_operation.FederatedTransformation() Interface defining the method for applying an operation over FederatedData","title":"FederatedTransformation class"},{"location":"private/federated_operation/#federatedtransformation-methods","text":"","title":"FederatedTransformation methods"},{"location":"private/federated_operation/#apply","text":"apply(data) This method receives data to be modified and performs the required modifications over it. Arguments: data : The object that has to be modified [source]","title":"apply"},{"location":"private/federated_operation/#normalize-class","text":"shfl.private.federated_operation.Normalize(mean, std) Normalization class of federated data FederatedData . It implements FederatedTransformation . Arguments: mean : mean used for normalization. std : std used for normalization.","title":"Normalize class"},{"location":"private/federated_operation/#federate_array","text":"shfl.private.federated_operation.federate_array(array, num_data_nodes) Creates FederatedData from an indexable array. The array will be divided using the first dimension. It supports Adaptive Differential Privacy through Privacy Filters Arguments: array : Indexable array with any number of dimensions num_data_nodes : Number of nodes to use Returns: federated_array : FederatedData with an array of size len(array)/num_data_nodes in every node","title":"federate_array"},{"location":"private/federated_operation/#apply_federated_transformation","text":"shfl.private.federated_operation.apply_federated_transformation(federated_data, federated_transformation) Applies the federated transformation over this federated data. Original federated data will be modified. Arguments: federated_data : FederatedData to use in the transformation federated_transformation : FederatedTransformation that will be applied over this data","title":"apply_federated_transformation"},{"location":"private/federated_operation/#split_train_test_1","text":"shfl.private.federated_operation.split_train_test(federated_data, test_split=0.2) Splits all data nodes in train and test sets Arguments: federated_data : FederatedData test_split : percentage of test split","title":"split_train_test"},{"location":"private/overview/","text":"Private This package contains most of the core elements of the framework that are used in almost every code that you will write with Sherpa.ai Federated Learning Framework. Maybe the most important element in the framework is the DataNode . A DataNode represents a device or element containing private data. In real world scenarios this data is typically property of a user or company. This data is private and access must be defined to be used. In this framework the definition is done through DataAccessDefinition , a function that is applied to data before share private information with someone out of the node. There is an special class of access where there is no access restrictions to the private data, UnprotectedAccess .","title":"Overview"},{"location":"private/overview/#private","text":"This package contains most of the core elements of the framework that are used in almost every code that you will write with Sherpa.ai Federated Learning Framework. Maybe the most important element in the framework is the DataNode . A DataNode represents a device or element containing private data. In real world scenarios this data is typically property of a user or company. This data is private and access must be defined to be used. In this framework the definition is done through DataAccessDefinition , a function that is applied to data before share private information with someone out of the node. There is an special class of access where there is no access restrictions to the private data, UnprotectedAccess .","title":"Private"},{"location":"private/query/","text":"[source] Query class shfl.private.query.Query() This class represents a query over private data. This interface exposes a method receiving data and must return a result based on this input. Query methods get get(data) Receives data and apply some function to answer it. Arguments: data : Data to process Returns: answer : Result of apply query over data [source] IdentityFunction class shfl.private.query.IdentityFunction() This function doesn't transform data. The answer is the data. [source] Mean class shfl.private.query.Mean() Implements mean over data array.","title":"Query"},{"location":"private/query/#query-class","text":"shfl.private.query.Query() This class represents a query over private data. This interface exposes a method receiving data and must return a result based on this input.","title":"Query class"},{"location":"private/query/#query-methods","text":"","title":"Query methods"},{"location":"private/query/#get","text":"get(data) Receives data and apply some function to answer it. Arguments: data : Data to process Returns: answer : Result of apply query over data [source]","title":"get"},{"location":"private/query/#identityfunction-class","text":"shfl.private.query.IdentityFunction() This function doesn't transform data. The answer is the data. [source]","title":"IdentityFunction class"},{"location":"private/query/#mean-class","text":"shfl.private.query.Mean() Implements mean over data array.","title":"Mean class"},{"location":"private/reproducibility/","text":"[source] Reproducibility shfl.private.reproducibility.Reproducibility(seed=None) Singleton class for ensure reproducibility. You indicates the seed and the execution is the same. The server initialice this class and the clients only call/get a seed. Server initialize it with Reproducibility(seed) before all executions For get a seed, the client has to put Reproducibility.get_instance().set_seed(ID) Is important to know that the reproducibility only works if you execute the experiment in CPU. Many ops in GPU like convolutions are not deterministic and the don't replicate. Arguments: seed : the main seed for server Properties: seed : return server seed seeds : return all seeds get_instance get_instance() Static access method. Returns: instance : Singleton instance class set_seed set_seed(id) Set server and clients seed Arguments: id : 'server' in server node and ID in client node delete_instance delete_instance() Remove the singleton instance. Not recommended for normal use. This method is necessary for tests.","title":"Reproducibility"},{"location":"private/reproducibility/#reproducibility","text":"shfl.private.reproducibility.Reproducibility(seed=None) Singleton class for ensure reproducibility. You indicates the seed and the execution is the same. The server initialice this class and the clients only call/get a seed. Server initialize it with Reproducibility(seed) before all executions For get a seed, the client has to put Reproducibility.get_instance().set_seed(ID) Is important to know that the reproducibility only works if you execute the experiment in CPU. Many ops in GPU like convolutions are not deterministic and the don't replicate. Arguments: seed : the main seed for server Properties: seed : return server seed seeds : return all seeds","title":"Reproducibility"},{"location":"private/reproducibility/#get_instance","text":"get_instance() Static access method. Returns: instance : Singleton instance class","title":"get_instance"},{"location":"private/reproducibility/#set_seed","text":"set_seed(id) Set server and clients seed Arguments: id : 'server' in server node and ID in client node","title":"set_seed"},{"location":"private/reproducibility/#delete_instance","text":"delete_instance() Remove the singleton instance. Not recommended for normal use. This method is necessary for tests.","title":"delete_instance"}]}